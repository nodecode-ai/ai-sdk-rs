{"id":"ai-sdk-rs-101","title":"Add common provider-id alias registrations for openai-compatible","description":"# atomic bead\n\n## full plan snapshot\nplan title: Provider Expansion Plan: Catalog-Normalized + ai-sdk-rs Registry Coverage\nplan summary: Normalize provider sdk labels at the Nodecode gateway appendix source and expand ai-sdk-rs provider routing so catalog providers resolve through existing implementations, starting with explicit Groq coverage and common OpenAI-compatible provider IDs.\n\n### integration points (complete map)\n- [ip-upstream-catalog-normalization] Gateway appendix sdk normalization\n  purpose: Guarantee emitted provider sdk values are canonical for ai-sdk-rs routing.\n  integration contract: appendix providers[].sdk values must be from the ai-sdk-rs-supported sdk set and preserve groq as groq.\n- [ip-openai-compatible-provider-id-routing] OpenAI-compatible provider-id routing in ai-sdk-rs\n  purpose: Resolve common provider IDs directly to OpenAI-compatible builder behavior.\n  integration contract: sdk_type_from_id and registry coverage must include target provider IDs with deterministic routing.\n- [ip-groq-routing] Explicit Groq routing support\n  purpose: Route sdk_type=groq catalog providers through a supported builder path.\n  integration contract: Groq provider definition must build a language model without fallback ambiguity.\n- [ip-nodecode-catalog-path] Nodecode catalog path precedence\n  purpose: Align runtime catalog reads to ~/.cache/nodecode/providers/catalog.json with safe fallback.\n  integration contract: path/env precedence is deterministic and test-covered for both nodecode and legacy paths.\n- [ip-fixture-parity-coverage] Fixture-based parity coverage for expanded provider IDs\n  purpose: Prevent request/stream/error mapping regressions while widening provider ID coverage.\n  integration contract: offline fixtures assert request serialization, stream normalization, and error mapping invariants.\n\n### global constraints\n- Preserve existing provider behavior for openai/anthropic/google/google-vertex/amazon-bedrock.\n- Use fixture-based parity tests only; no live provider API calls.\n- Keep Rust changes limited to currently representable interfaces: language, embeddings, images.\n- Any provider sdk label emitted by appendix must be parseable and routable by ai-sdk-rs without ad-hoc fallbacks.\n\n### definition of done\n- Gateway appendix export emits normalized sdk labels compatible with ai-sdk-rs.\n- ai-sdk-rs resolves and builds models for Groq and selected OpenAI-compatible provider IDs.\n- Capabilities loader prefers ~/.cache/nodecode/providers/catalog.json with env override precedence.\n- New regression tests cover sdk normalization, registry resolution, path precedence, and provider parity fixtures.\n- Dependency graph has no cycles and all created beads are ready/traceable.\n\n### plan assumptions\n- Catalog source of truth is generated in ~/nodecode-gateway-1 via modrepo appendix tooling.\n- Provider-specific TS surfaces without Rust interfaces (for example transcription/reranking) are explicitly deferred.\n- openai-compatible Rust implementation remains the shared execution path for aliased provider IDs.\n\n### plan risks\n- Unnormalized sdk labels can silently break provider dispatch at runtime.\n- Alias expansion may regress provider option scoping if provider_scope_name handling drifts.\n- Cache path changes can cause environment-specific behavior if precedence is not tested.\n\n## bead-local scope\nbead key: ai-sdk-rs-openai-compatible-aliases\ntask: Add ProviderRegistration aliases for xai, deepseek, mistral, togetherai, fireworks-ai, deepinfra, openrouter, and perplexity mapped to SdkType::OpenAICompatible.\nintegration points touched: ip-openai-compatible-provider-id-routing\nwhy: Direct provider-id routing improves deterministic resolution against active catalog entries.\n\n### deliverables\n- alias registration set for targeted provider IDs\n- consistent builder selection through openai-compatible path\n\n### acceptance\n- sdk_type_from_id returns OpenAICompatible for each target alias\n- model build path resolves without unsupported-provider errors\n\n### dependency intent\n- provider-expansion-baseline-matrix\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:3086162 picked-at=2026-02-14T04:51:45Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-101.lock","status":"closed","priority":1,"issue_type":"feature","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T20:33:09.001089684-08:00","created_by":"pomterre","updated_at":"2026-02-13T20:56:07.74493745-08:00","closed_at":"2026-02-13T20:56:07.74493745-08:00","close_reason":"Added openai-compatible alias registry tests for sdk_type_from_id and model build coverage across xai/deepseek/mistral/togetherai/fireworks-ai/deepinfra/openrouter/perplexity; alias registrations already present in provider registry.","labels":["ai-sdk-rs","provider-routing"],"dependencies":[{"issue_id":"ai-sdk-rs-101","depends_on_id":"ai-sdk-rs-am3","type":"blocks","created_at":"2026-02-13T20:33:09.465454712-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-1gj","title":"Harden stream failed-finish fixture assertions","description":"# atomic bead\n\n## full plan snapshot\nplan title: OpenAI Responses parity gap closure (atomized)\nplan summary: Close the OpenAI Responses parity gaps against /home/mike/ai/packages/openai/src/responses by landing error-path, finish semantics, usage mapping, and function tool strict-mode parity with fixture-backed regressions.\n\n### integration points (complete map)\n- [ip-nonstream-response-error] Non-stream top-level error behavior\n  purpose: Ensure do_generate fails when response payload contains top-level error, matching TS behavior.\n  integration contract: If response.error exists, return SdkError::Upstream instead of GenerateResponse.\n- [ip-stream-failed-finish] Stream failed-finish semantics\n  purpose: Align response.failed stream trajectories with TS finish semantics and metadata invariants.\n  integration contract: response.failed must not be treated as response.completed/incomplete; finish reason remains Other with correct metadata behavior.\n- [ip-usage-details] Nested usage details mapping\n  purpose: Map cached and reasoning token details from nested provider usage structures.\n  integration contract: input_tokens_details.cached_tokens -\u003e Usage.cached_input_tokens and output_tokens_details.reasoning_tokens -\u003e Usage.reasoning_tokens.\n- [ip-function-tool-strict] Function tool strict passthrough\n  purpose: Preserve strict true/false/unset semantics in function tool request serialization.\n  integration contract: Serialize strict when explicitly true/false and omit when unset, matching TS prepare-tools behavior.\n\n### global constraints\n- Existing provider only: openai responses path in ai-sdk-rs; do not introduce new providers.\n- Prefer fixture-based tests; do not call live provider APIs.\n- Preserve normalized StreamPart/Event semantics and keep TS baseline behavior authoritative.\n- Keep changes scoped to openai provider and sdk-types only where required for strict parity.\n\n### definition of done\n- All beads reference at least one integration point and can execute independently with local context.\n- OpenAI parity regressions for the targeted behaviors are covered by deterministic tests.\n- cargo test --lib provider_openai:: passes for updated tests.\n- Dependency graph has no cycles and intended blocker relationships are encoded.\n\n### plan assumptions\n- docs/plans/openai-responses-parity-gap-matrix.md is the source plan for this atomization.\n- Fixture files under crates/providers/openai/tests/fixtures remain available and stable.\n- TS baseline files under /home/mike/ai/packages/openai/src/responses remain the parity reference.\n\n### plan risks\n- Finish-reason handling changes can regress stream ordering if hooks are altered incorrectly.\n- Usage mapping can silently drift if nested token fields are not tested in both stream and non-stream paths.\n- Function strict-mode parity can regress if provider options fallback and typed field behavior diverge.\n\n## bead-local scope\nbead key: stream-failed-finish-fixture-tests\ntask: Extend fixture tests to assert ordering, finish reason, and provider metadata for openai-error stream fixture.\nintegration points touched: ip-stream-failed-finish\nwhy: Prevents future drift in subtle stream terminal semantics.\n\n### deliverables\n- Fixture assertions for error-before-finish ordering\n- Assertions for FinishReason::Other and responseId/serviceTier expectations\n\n### acceptance\n- Tests fail under incorrect response.failed handling\n- Tests pass with aligned mapper behavior\n\n### dependency intent\n- stream-failed-finish-impl\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:2906418 picked-at=2026-02-14T04:09:35Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-1gj.lock","status":"closed","priority":1,"issue_type":"task","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T19:59:48.789583994-08:00","created_by":"pomterre","updated_at":"2026-02-13T20:10:37.954195104-08:00","closed_at":"2026-02-13T20:10:37.954195104-08:00","close_reason":"Expanded openai-error stream fixture assertions for exact error/finish counts, responseId consistency, and exact fixture message parity","labels":["fixtures","openai","parity","stream","tests"],"dependencies":[{"issue_id":"ai-sdk-rs-1gj","depends_on_id":"ai-sdk-rs-650","type":"blocks","created_at":"2026-02-13T19:59:49.389414761-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-1ku","title":"Expand Google/Vertex parity regression suite","description":"# atomic bead\n\n## full plan snapshot\nplan title: ai-sdk-rs fragility hardening and architecture LOC reduction\nplan summary: Harden known fragility points and reduce architecture duplication in provider stacks while preserving behavior parity and preventing coupling regression.\n\n### integration points (complete map)\n- [ip-openai-failure-semantics] OpenAI failure semantics\n  purpose: Ensure non-stream and stream failure paths return correct error/finish behavior.\n  integration contract: Non-stream top-level error must not return GenerateResponse success; stream failed trajectories must emit parity-aligned finish/error semantics.\n- [ip-transport-init-fallibility] Transport initialization fallibility\n  purpose: Prevent SDK process aborts from transport client init failures.\n  integration contract: Reqwest transport initialization surfaces recoverable errors instead of panicking.\n- [ip-google-stack-consolidation] Google and Vertex stack consolidation\n  purpose: Collapse duplicated provider implementations without behavior drift.\n  integration contract: Shared code preserves provider-specific auth, options scopes, and stream normalization semantics.\n- [ip-provider-bootstrap-dedupe] Provider bootstrap deduplication\n  purpose: Reduce repeated builder/header/default-option wiring across providers.\n  integration contract: Credential/env/header precedence and endpoint defaulting remain identical per provider.\n- [ip-error-mapping-unification] Error mapping unification\n  purpose: Unify fragmented error mapping with consistent diagnostics.\n  integration contract: Provider-specific parsing remains intact while fallback diagnostics become consistent and actionable.\n- [ip-module-boundary-normalization] Module boundary normalization\n  purpose: Reduce path-based boundary indirection and improve coupling observability.\n  integration contract: Public module exposure remains stable while internal structure improves analyzability and maintainability.\n\n### global constraints\n- No behavior or functionality loss.\n- Preserve public API contracts unless explicitly approved.\n- Keep provider request/response shapes and stream-part ordering stable.\n- Do not accept any coupling regression at the workspace gate.\n- Each slice must be validated with tests and quant_metrics snapshot/compare.\n\n### definition of done\n- OpenAI failure semantics match expected non-stream and stream parity behavior.\n- Transport initialization no longer panics on client build failure paths.\n- Google and Google Vertex duplicated stacks are consolidated behind shared modules with parity maintained.\n- Provider bootstrap and error-mapping duplication are reduced with equivalent behavior.\n- Path-based module boundary debt is reduced enough for coupling metrics to remain trustworthy for future slices.\n- quant_metrics compare verdict remains improves or no-regression for accepted slices.\n\n### plan assumptions\n- Current parity gap matrix for OpenAI responses reflects active known gaps.\n- inventory-based provider registration remains required.\n- Refactors can be staged incrementally without full rewrite.\n\n### plan risks\n- Provider parity regressions caused by shared-module extraction.\n- Hidden differences between Google GenAI and Google Vertex behavior during consolidation.\n- Coupling metric fidelity is partially limited by current path-based module structure.\n\n## bead-local scope\nbead key: google-vertex-parity-regression-suite\ntask: Add cross-provider parity assertions for shared paths to detect future divergence after consolidation.\nintegration points touched: ip-google-stack-consolidation\nwhy: n/a\n\n### deliverables\n- Parity tests across shared Google/Vertex request and stream behaviors\n- Coverage for provider-option scope and thought-signature metadata differences\n\n### acceptance\n- Regression tests fail on intentional divergence and pass on aligned behavior\n- CI-visible safety net for future refactors\n\n### dependency intent\n- google-vertex-language-model-core\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:2535404 picked-at=2026-02-14T03:02:31Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-1ku.lock","status":"closed","priority":1,"issue_type":"task","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T18:24:56.463388724-08:00","created_by":"pomterre","updated_at":"2026-02-13T19:21:10.755779711-08:00","closed_at":"2026-02-13T19:21:10.755779711-08:00","close_reason":"Added Google/Vertex parity regression suite coverage and verified with workspace tests.","labels":["google","parity","tests"],"dependencies":[{"issue_id":"ai-sdk-rs-1ku","depends_on_id":"ai-sdk-rs-r9t","type":"blocks","created_at":"2026-02-13T18:25:44.766416983-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-2v2","title":"Normalize path-based module boundaries incrementally","description":"# atomic bead\n\n## full plan snapshot\nplan title: ai-sdk-rs fragility hardening and architecture LOC reduction\nplan summary: Harden known fragility points and reduce architecture duplication in provider stacks while preserving behavior parity and preventing coupling regression.\n\n### integration points (complete map)\n- [ip-openai-failure-semantics] OpenAI failure semantics\n  purpose: Ensure non-stream and stream failure paths return correct error/finish behavior.\n  integration contract: Non-stream top-level error must not return GenerateResponse success; stream failed trajectories must emit parity-aligned finish/error semantics.\n- [ip-transport-init-fallibility] Transport initialization fallibility\n  purpose: Prevent SDK process aborts from transport client init failures.\n  integration contract: Reqwest transport initialization surfaces recoverable errors instead of panicking.\n- [ip-google-stack-consolidation] Google and Vertex stack consolidation\n  purpose: Collapse duplicated provider implementations without behavior drift.\n  integration contract: Shared code preserves provider-specific auth, options scopes, and stream normalization semantics.\n- [ip-provider-bootstrap-dedupe] Provider bootstrap deduplication\n  purpose: Reduce repeated builder/header/default-option wiring across providers.\n  integration contract: Credential/env/header precedence and endpoint defaulting remain identical per provider.\n- [ip-error-mapping-unification] Error mapping unification\n  purpose: Unify fragmented error mapping with consistent diagnostics.\n  integration contract: Provider-specific parsing remains intact while fallback diagnostics become consistent and actionable.\n- [ip-module-boundary-normalization] Module boundary normalization\n  purpose: Reduce path-based boundary indirection and improve coupling observability.\n  integration contract: Public module exposure remains stable while internal structure improves analyzability and maintainability.\n\n### global constraints\n- No behavior or functionality loss.\n- Preserve public API contracts unless explicitly approved.\n- Keep provider request/response shapes and stream-part ordering stable.\n- Do not accept any coupling regression at the workspace gate.\n- Each slice must be validated with tests and quant_metrics snapshot/compare.\n\n### definition of done\n- OpenAI failure semantics match expected non-stream and stream parity behavior.\n- Transport initialization no longer panics on client build failure paths.\n- Google and Google Vertex duplicated stacks are consolidated behind shared modules with parity maintained.\n- Provider bootstrap and error-mapping duplication are reduced with equivalent behavior.\n- Path-based module boundary debt is reduced enough for coupling metrics to remain trustworthy for future slices.\n- quant_metrics compare verdict remains improves or no-regression for accepted slices.\n\n### plan assumptions\n- Current parity gap matrix for OpenAI responses reflects active known gaps.\n- inventory-based provider registration remains required.\n- Refactors can be staged incrementally without full rewrite.\n\n### plan risks\n- Provider parity regressions caused by shared-module extraction.\n- Hidden differences between Google GenAI and Google Vertex behavior during consolidation.\n- Coupling metric fidelity is partially limited by current path-based module structure.\n\n## bead-local scope\nbead key: module-boundary-normalization-slice\ntask: Reduce path-indirection in crate wiring to improve ownership clarity and coupling analyzability while keeping public exports stable.\nintegration points touched: ip-module-boundary-normalization\nwhy: Current path-mod setup obscures boundary metrics and increases maintenance complexity.\n\n### deliverables\n- Incremental boundary normalization plan executed for one stable slice\n- Updated docs/comments on module boundary ownership\n\n### acceptance\n- Public-facing module access paths remain stable\n- Tooling warnings related to unresolved path modules are reduced for the slice\n\n### dependency intent\n- provider-error-mapper-unification\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:2646177 picked-at=2026-02-14T03:28:39Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-2v2.lock","status":"closed","priority":3,"issue_type":"chore","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T18:24:56.716021837-08:00","created_by":"pomterre","updated_at":"2026-02-13T19:32:18.885184968-08:00","closed_at":"2026-02-13T19:32:18.885184968-08:00","close_reason":"Added google-vertex shared boundary facade and rerouted path-indirected imports through a local ownership seam","labels":["architecture","maintainability"],"dependencies":[{"issue_id":"ai-sdk-rs-2v2","depends_on_id":"ai-sdk-rs-ke2","type":"blocks","created_at":"2026-02-13T18:25:45.0205819-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-2zp","title":"Run quant coupling and LOC gates for each refactor slice","description":"# atomic bead\n\n## full plan snapshot\nplan title: ai-sdk-rs fragility hardening and architecture LOC reduction\nplan summary: Harden known fragility points and reduce architecture duplication in provider stacks while preserving behavior parity and preventing coupling regression.\n\n### integration points (complete map)\n- [ip-openai-failure-semantics] OpenAI failure semantics\n  purpose: Ensure non-stream and stream failure paths return correct error/finish behavior.\n  integration contract: Non-stream top-level error must not return GenerateResponse success; stream failed trajectories must emit parity-aligned finish/error semantics.\n- [ip-transport-init-fallibility] Transport initialization fallibility\n  purpose: Prevent SDK process aborts from transport client init failures.\n  integration contract: Reqwest transport initialization surfaces recoverable errors instead of panicking.\n- [ip-google-stack-consolidation] Google and Vertex stack consolidation\n  purpose: Collapse duplicated provider implementations without behavior drift.\n  integration contract: Shared code preserves provider-specific auth, options scopes, and stream normalization semantics.\n- [ip-provider-bootstrap-dedupe] Provider bootstrap deduplication\n  purpose: Reduce repeated builder/header/default-option wiring across providers.\n  integration contract: Credential/env/header precedence and endpoint defaulting remain identical per provider.\n- [ip-error-mapping-unification] Error mapping unification\n  purpose: Unify fragmented error mapping with consistent diagnostics.\n  integration contract: Provider-specific parsing remains intact while fallback diagnostics become consistent and actionable.\n- [ip-module-boundary-normalization] Module boundary normalization\n  purpose: Reduce path-based boundary indirection and improve coupling observability.\n  integration contract: Public module exposure remains stable while internal structure improves analyzability and maintainability.\n\n### global constraints\n- No behavior or functionality loss.\n- Preserve public API contracts unless explicitly approved.\n- Keep provider request/response shapes and stream-part ordering stable.\n- Do not accept any coupling regression at the workspace gate.\n- Each slice must be validated with tests and quant_metrics snapshot/compare.\n\n### definition of done\n- OpenAI failure semantics match expected non-stream and stream parity behavior.\n- Transport initialization no longer panics on client build failure paths.\n- Google and Google Vertex duplicated stacks are consolidated behind shared modules with parity maintained.\n- Provider bootstrap and error-mapping duplication are reduced with equivalent behavior.\n- Path-based module boundary debt is reduced enough for coupling metrics to remain trustworthy for future slices.\n- quant_metrics compare verdict remains improves or no-regression for accepted slices.\n\n### plan assumptions\n- Current parity gap matrix for OpenAI responses reflects active known gaps.\n- inventory-based provider registration remains required.\n- Refactors can be staged incrementally without full rewrite.\n\n### plan risks\n- Provider parity regressions caused by shared-module extraction.\n- Hidden differences between Google GenAI and Google Vertex behavior during consolidation.\n- Coupling metric fidelity is partially limited by current path-based module structure.\n\n## bead-local scope\nbead key: quant-gate-after-each-slice\ntask: Enforce per-slice quant_metrics snapshot/compare and reject any slice that regresses coupling gate verdict.\nintegration points touched: ip-openai-failure-semantics, ip-transport-init-fallibility, ip-google-stack-consolidation, ip-provider-bootstrap-dedupe, ip-error-mapping-unification, ip-module-boundary-normalization\nwhy: Maintains hard no-regression architecture safety while deleting code.\n\n### deliverables\n- Before/after snapshot artifacts per accepted slice\n- Gate verdict log for all slices\n\n### acceptance\n- All accepted slices show compare verdict improves or no-regression\n- No slice merged without quantitative checkpoint evidence\n\n### dependency intent\n- openai-nonstream-error-gate\n- openai-stream-failed-finish-fix\n- reqwest-init-failure-tests\n- google-vertex-parity-regression-suite\n- module-boundary-normalization-slice\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:2680358 picked-at=2026-02-14T03:32:56Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-2zp.lock","status":"closed","priority":0,"issue_type":"task","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T18:25:32.764723306-08:00","created_by":"pomterre","updated_at":"2026-02-13T19:37:07.524253989-08:00","closed_at":"2026-02-13T19:37:07.524253989-08:00","close_reason":"Published per-slice quant gate artifacts in docs/metrics and verified all required slices as improves/no-regression with targeted test evidence","labels":["coupling","gate","metrics","quality-gate"],"dependencies":[{"issue_id":"ai-sdk-rs-2zp","depends_on_id":"ai-sdk-rs-4kn","type":"blocks","created_at":"2026-02-13T18:25:45.113637566-08:00","created_by":"pomterre"},{"issue_id":"ai-sdk-rs-2zp","depends_on_id":"ai-sdk-rs-mpj","type":"blocks","created_at":"2026-02-13T18:25:45.212238377-08:00","created_by":"pomterre"},{"issue_id":"ai-sdk-rs-2zp","depends_on_id":"ai-sdk-rs-gsu","type":"blocks","created_at":"2026-02-13T18:25:45.309112023-08:00","created_by":"pomterre"},{"issue_id":"ai-sdk-rs-2zp","depends_on_id":"ai-sdk-rs-1ku","type":"blocks","created_at":"2026-02-13T18:25:45.400746072-08:00","created_by":"pomterre"},{"issue_id":"ai-sdk-rs-2zp","depends_on_id":"ai-sdk-rs-2v2","type":"blocks","created_at":"2026-02-13T18:25:45.495852395-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-3v9","title":"Expand ai-sdk-rs registry tests for Groq and provider aliases","description":"# atomic bead\n\n## full plan snapshot\nplan title: Provider Expansion Plan: Catalog-Normalized + ai-sdk-rs Registry Coverage\nplan summary: Normalize provider sdk labels at the Nodecode gateway appendix source and expand ai-sdk-rs provider routing so catalog providers resolve through existing implementations, starting with explicit Groq coverage and common OpenAI-compatible provider IDs.\n\n### integration points (complete map)\n- [ip-upstream-catalog-normalization] Gateway appendix sdk normalization\n  purpose: Guarantee emitted provider sdk values are canonical for ai-sdk-rs routing.\n  integration contract: appendix providers[].sdk values must be from the ai-sdk-rs-supported sdk set and preserve groq as groq.\n- [ip-openai-compatible-provider-id-routing] OpenAI-compatible provider-id routing in ai-sdk-rs\n  purpose: Resolve common provider IDs directly to OpenAI-compatible builder behavior.\n  integration contract: sdk_type_from_id and registry coverage must include target provider IDs with deterministic routing.\n- [ip-groq-routing] Explicit Groq routing support\n  purpose: Route sdk_type=groq catalog providers through a supported builder path.\n  integration contract: Groq provider definition must build a language model without fallback ambiguity.\n- [ip-nodecode-catalog-path] Nodecode catalog path precedence\n  purpose: Align runtime catalog reads to ~/.cache/nodecode/providers/catalog.json with safe fallback.\n  integration contract: path/env precedence is deterministic and test-covered for both nodecode and legacy paths.\n- [ip-fixture-parity-coverage] Fixture-based parity coverage for expanded provider IDs\n  purpose: Prevent request/stream/error mapping regressions while widening provider ID coverage.\n  integration contract: offline fixtures assert request serialization, stream normalization, and error mapping invariants.\n\n### global constraints\n- Preserve existing provider behavior for openai/anthropic/google/google-vertex/amazon-bedrock.\n- Use fixture-based parity tests only; no live provider API calls.\n- Keep Rust changes limited to currently representable interfaces: language, embeddings, images.\n- Any provider sdk label emitted by appendix must be parseable and routable by ai-sdk-rs without ad-hoc fallbacks.\n\n### definition of done\n- Gateway appendix export emits normalized sdk labels compatible with ai-sdk-rs.\n- ai-sdk-rs resolves and builds models for Groq and selected OpenAI-compatible provider IDs.\n- Capabilities loader prefers ~/.cache/nodecode/providers/catalog.json with env override precedence.\n- New regression tests cover sdk normalization, registry resolution, path precedence, and provider parity fixtures.\n- Dependency graph has no cycles and all created beads are ready/traceable.\n\n### plan assumptions\n- Catalog source of truth is generated in ~/nodecode-gateway-1 via modrepo appendix tooling.\n- Provider-specific TS surfaces without Rust interfaces (for example transcription/reranking) are explicitly deferred.\n- openai-compatible Rust implementation remains the shared execution path for aliased provider IDs.\n\n### plan risks\n- Unnormalized sdk labels can silently break provider dispatch at runtime.\n- Alias expansion may regress provider option scoping if provider_scope_name handling drifts.\n- Cache path changes can cause environment-specific behavior if precedence is not tested.\n\n## bead-local scope\nbead key: ai-sdk-rs-registry-routing-tests\ntask: Extend openai-compatible/provider registry tests to assert registration presence, sdk_type mapping, and successful model build for Groq and target aliases.\nintegration points touched: ip-openai-compatible-provider-id-routing, ip-groq-routing\nwhy: Registry behavior is critical and must remain explicit under future provider additions.\n\n### deliverables\n- test cases covering Groq and each new alias id\n- assertions on provider_name/model_id from built models\n\n### acceptance\n- tests fail on missing or mis-typed registrations\n- tests enforce deterministic routing semantics\n\n### dependency intent\n- ai-sdk-rs-groq-registration\n- ai-sdk-rs-openai-compatible-aliases\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","status":"open","priority":1,"issue_type":"task","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T20:33:09.065578554-08:00","created_by":"pomterre","updated_at":"2026-02-13T20:33:09.065578554-08:00","labels":["ai-sdk-rs","tests"],"dependencies":[{"issue_id":"ai-sdk-rs-3v9","depends_on_id":"ai-sdk-rs-oix","type":"blocks","created_at":"2026-02-13T20:33:09.509948801-08:00","created_by":"pomterre"},{"issue_id":"ai-sdk-rs-3v9","depends_on_id":"ai-sdk-rs-101","type":"blocks","created_at":"2026-02-13T20:33:09.556314837-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-45c","title":"Implement non-stream response.error guard in do_generate","description":"# atomic bead\n\n## full plan snapshot\nplan title: OpenAI Responses parity gap closure (atomized)\nplan summary: Close the OpenAI Responses parity gaps against /home/mike/ai/packages/openai/src/responses by landing error-path, finish semantics, usage mapping, and function tool strict-mode parity with fixture-backed regressions.\n\n### integration points (complete map)\n- [ip-nonstream-response-error] Non-stream top-level error behavior\n  purpose: Ensure do_generate fails when response payload contains top-level error, matching TS behavior.\n  integration contract: If response.error exists, return SdkError::Upstream instead of GenerateResponse.\n- [ip-stream-failed-finish] Stream failed-finish semantics\n  purpose: Align response.failed stream trajectories with TS finish semantics and metadata invariants.\n  integration contract: response.failed must not be treated as response.completed/incomplete; finish reason remains Other with correct metadata behavior.\n- [ip-usage-details] Nested usage details mapping\n  purpose: Map cached and reasoning token details from nested provider usage structures.\n  integration contract: input_tokens_details.cached_tokens -\u003e Usage.cached_input_tokens and output_tokens_details.reasoning_tokens -\u003e Usage.reasoning_tokens.\n- [ip-function-tool-strict] Function tool strict passthrough\n  purpose: Preserve strict true/false/unset semantics in function tool request serialization.\n  integration contract: Serialize strict when explicitly true/false and omit when unset, matching TS prepare-tools behavior.\n\n### global constraints\n- Existing provider only: openai responses path in ai-sdk-rs; do not introduce new providers.\n- Prefer fixture-based tests; do not call live provider APIs.\n- Preserve normalized StreamPart/Event semantics and keep TS baseline behavior authoritative.\n- Keep changes scoped to openai provider and sdk-types only where required for strict parity.\n\n### definition of done\n- All beads reference at least one integration point and can execute independently with local context.\n- OpenAI parity regressions for the targeted behaviors are covered by deterministic tests.\n- cargo test --lib provider_openai:: passes for updated tests.\n- Dependency graph has no cycles and intended blocker relationships are encoded.\n\n### plan assumptions\n- docs/plans/openai-responses-parity-gap-matrix.md is the source plan for this atomization.\n- Fixture files under crates/providers/openai/tests/fixtures remain available and stable.\n- TS baseline files under /home/mike/ai/packages/openai/src/responses remain the parity reference.\n\n### plan risks\n- Finish-reason handling changes can regress stream ordering if hooks are altered incorrectly.\n- Usage mapping can silently drift if nested token fields are not tested in both stream and non-stream paths.\n- Function strict-mode parity can regress if provider options fallback and typed field behavior diverge.\n\n## bead-local scope\nbead key: nonstream-error-impl-guard\ntask: Implement/maintain early response.error guard in do_generate before content extraction and usage mapping.\nintegration points touched: ip-nonstream-response-error\nwhy: Ensures non-stream behavior matches TS APICallError semantics in Rust error surface.\n\n### deliverables\n- Guarded error path in OpenAI responses language model non-stream flow\n- Consistent SdkError::Upstream mapping for top-level error payloads\n\n### acceptance\n- do_generate never returns GenerateResponse when response.error exists\n- Regression test remains green\n\n### dependency intent\n- nonstream-error-regression-contract\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:2897052 picked-at=2026-02-14T04:06:37Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-45c.lock","status":"closed","priority":0,"issue_type":"task","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T19:59:48.616349069-08:00","created_by":"pomterre","updated_at":"2026-02-13T20:07:54.153983727-08:00","closed_at":"2026-02-13T20:07:54.153983727-08:00","close_reason":"Validated and maintained early do_generate response.error guard before extraction/mapping; parity behavior confirmed by non-stream regression test","labels":["impl","nonstream","openai","parity"],"dependencies":[{"issue_id":"ai-sdk-rs-45c","depends_on_id":"ai-sdk-rs-cpi","type":"blocks","created_at":"2026-02-13T19:59:49.264107391-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-486","title":"Add regression tests for appendix sdk normalization","description":"# atomic bead\n\n## full plan snapshot\nplan title: Provider Expansion Plan: Catalog-Normalized + ai-sdk-rs Registry Coverage\nplan summary: Normalize provider sdk labels at the Nodecode gateway appendix source and expand ai-sdk-rs provider routing so catalog providers resolve through existing implementations, starting with explicit Groq coverage and common OpenAI-compatible provider IDs.\n\n### integration points (complete map)\n- [ip-upstream-catalog-normalization] Gateway appendix sdk normalization\n  purpose: Guarantee emitted provider sdk values are canonical for ai-sdk-rs routing.\n  integration contract: appendix providers[].sdk values must be from the ai-sdk-rs-supported sdk set and preserve groq as groq.\n- [ip-openai-compatible-provider-id-routing] OpenAI-compatible provider-id routing in ai-sdk-rs\n  purpose: Resolve common provider IDs directly to OpenAI-compatible builder behavior.\n  integration contract: sdk_type_from_id and registry coverage must include target provider IDs with deterministic routing.\n- [ip-groq-routing] Explicit Groq routing support\n  purpose: Route sdk_type=groq catalog providers through a supported builder path.\n  integration contract: Groq provider definition must build a language model without fallback ambiguity.\n- [ip-nodecode-catalog-path] Nodecode catalog path precedence\n  purpose: Align runtime catalog reads to ~/.cache/nodecode/providers/catalog.json with safe fallback.\n  integration contract: path/env precedence is deterministic and test-covered for both nodecode and legacy paths.\n- [ip-fixture-parity-coverage] Fixture-based parity coverage for expanded provider IDs\n  purpose: Prevent request/stream/error mapping regressions while widening provider ID coverage.\n  integration contract: offline fixtures assert request serialization, stream normalization, and error mapping invariants.\n\n### global constraints\n- Preserve existing provider behavior for openai/anthropic/google/google-vertex/amazon-bedrock.\n- Use fixture-based parity tests only; no live provider API calls.\n- Keep Rust changes limited to currently representable interfaces: language, embeddings, images.\n- Any provider sdk label emitted by appendix must be parseable and routable by ai-sdk-rs without ad-hoc fallbacks.\n\n### definition of done\n- Gateway appendix export emits normalized sdk labels compatible with ai-sdk-rs.\n- ai-sdk-rs resolves and builds models for Groq and selected OpenAI-compatible provider IDs.\n- Capabilities loader prefers ~/.cache/nodecode/providers/catalog.json with env override precedence.\n- New regression tests cover sdk normalization, registry resolution, path precedence, and provider parity fixtures.\n- Dependency graph has no cycles and all created beads are ready/traceable.\n\n### plan assumptions\n- Catalog source of truth is generated in ~/nodecode-gateway-1 via modrepo appendix tooling.\n- Provider-specific TS surfaces without Rust interfaces (for example transcription/reranking) are explicitly deferred.\n- openai-compatible Rust implementation remains the shared execution path for aliased provider IDs.\n\n### plan risks\n- Unnormalized sdk labels can silently break provider dispatch at runtime.\n- Alias expansion may regress provider option scoping if provider_scope_name handling drifts.\n- Cache path changes can cause environment-specific behavior if precedence is not tested.\n\n## bead-local scope\nbead key: gateway-appendix-sdk-normalization-tests\ntask: Extend modrepo CLI appendix tests to assert normalized sdk output for groq, xai, mistral, deepseek, openrouter, perplexity, togetherai, fireworks-ai, and deepinfra.\nintegration points touched: ip-upstream-catalog-normalization\nwhy: Normalization rules need stable automated protection against future registry drift.\n\n### deliverables\n- test fixtures/expectations for normalized providers[].sdk\n- coverage for mixed source sdk labels\n\n### acceptance\n- tests fail if unsupported sdk labels leak into appendix output\n- tests validate groq is not collapsed into openai-compatible\n\n### dependency intent\n- gateway-appendix-sdk-normalization\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","status":"open","priority":1,"issue_type":"task","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T20:33:08.886811093-08:00","created_by":"pomterre","updated_at":"2026-02-13T20:33:08.886811093-08:00","labels":["gateway","tests"],"dependencies":[{"issue_id":"ai-sdk-rs-486","depends_on_id":"ai-sdk-rs-v4c","type":"blocks","created_at":"2026-02-13T20:33:09.35872115-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-4kn","title":"Enforce non-stream OpenAI top-level error handling","description":"# atomic bead\n\n## full plan snapshot\nplan title: ai-sdk-rs fragility hardening and architecture LOC reduction\nplan summary: Harden known fragility points and reduce architecture duplication in provider stacks while preserving behavior parity and preventing coupling regression.\n\n### integration points (complete map)\n- [ip-openai-failure-semantics] OpenAI failure semantics\n  purpose: Ensure non-stream and stream failure paths return correct error/finish behavior.\n  integration contract: Non-stream top-level error must not return GenerateResponse success; stream failed trajectories must emit parity-aligned finish/error semantics.\n- [ip-transport-init-fallibility] Transport initialization fallibility\n  purpose: Prevent SDK process aborts from transport client init failures.\n  integration contract: Reqwest transport initialization surfaces recoverable errors instead of panicking.\n- [ip-google-stack-consolidation] Google and Vertex stack consolidation\n  purpose: Collapse duplicated provider implementations without behavior drift.\n  integration contract: Shared code preserves provider-specific auth, options scopes, and stream normalization semantics.\n- [ip-provider-bootstrap-dedupe] Provider bootstrap deduplication\n  purpose: Reduce repeated builder/header/default-option wiring across providers.\n  integration contract: Credential/env/header precedence and endpoint defaulting remain identical per provider.\n- [ip-error-mapping-unification] Error mapping unification\n  purpose: Unify fragmented error mapping with consistent diagnostics.\n  integration contract: Provider-specific parsing remains intact while fallback diagnostics become consistent and actionable.\n- [ip-module-boundary-normalization] Module boundary normalization\n  purpose: Reduce path-based boundary indirection and improve coupling observability.\n  integration contract: Public module exposure remains stable while internal structure improves analyzability and maintainability.\n\n### global constraints\n- No behavior or functionality loss.\n- Preserve public API contracts unless explicitly approved.\n- Keep provider request/response shapes and stream-part ordering stable.\n- Do not accept any coupling regression at the workspace gate.\n- Each slice must be validated with tests and quant_metrics snapshot/compare.\n\n### definition of done\n- OpenAI failure semantics match expected non-stream and stream parity behavior.\n- Transport initialization no longer panics on client build failure paths.\n- Google and Google Vertex duplicated stacks are consolidated behind shared modules with parity maintained.\n- Provider bootstrap and error-mapping duplication are reduced with equivalent behavior.\n- Path-based module boundary debt is reduced enough for coupling metrics to remain trustworthy for future slices.\n- quant_metrics compare verdict remains improves or no-regression for accepted slices.\n\n### plan assumptions\n- Current parity gap matrix for OpenAI responses reflects active known gaps.\n- inventory-based provider registration remains required.\n- Refactors can be staged incrementally without full rewrite.\n\n### plan risks\n- Provider parity regressions caused by shared-module extraction.\n- Hidden differences between Google GenAI and Google Vertex behavior during consolidation.\n- Coupling metric fidelity is partially limited by current path-based module structure.\n\n## bead-local scope\nbead key: openai-nonstream-error-gate\ntask: Update do_generate response handling so top-level error payloads return SdkError instead of GenerateResponse success.\nintegration points touched: ip-openai-failure-semantics\nwhy: Addresses highest-severity correctness risk in request lifecycle.\n\n### deliverables\n- Non-stream error-path guard with parity-aligned mapping\n- Updated tests validating returned error behavior\n\n### acceptance\n- Top-level error response no longer yields success payload\n- Existing success-path behavior unchanged\n\n### dependency intent\n- openai-failure-char-tests\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=codex picked-at=2026-02-14T02:30:36Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-4kn.lock","status":"closed","priority":0,"issue_type":"bug","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T18:24:55.914559288-08:00","created_by":"pomterre","updated_at":"2026-02-13T18:43:55.128943727-08:00","closed_at":"2026-02-13T18:43:55.128943727-08:00","close_reason":"Non-stream top-level response.error handling is already implemented and covered by regression test non_stream_response_error_returns_error (passing). Closing duplicate in-progress claim.","labels":["fragility","openai","parity"],"dependencies":[{"issue_id":"ai-sdk-rs-4kn","depends_on_id":"ai-sdk-rs-qw4","type":"blocks","created_at":"2026-02-13T18:25:44.398618151-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-62d","title":"Add regression tests for nested usage detail mapping","description":"# atomic bead\n\n## full plan snapshot\nplan title: OpenAI Responses parity remediation for ai-sdk-rs\nplan summary: Close behavior drift between ai-sdk-rs OpenAI Responses provider and /home/mike/ai by fixing non-stream error propagation, response.failed streaming finish semantics, usage detail mapping, and function tool strict passthrough, with fixture-based regression coverage.\n\n### integration points (complete map)\n- [ip-nonstream-response-error] Non-stream response.error propagation\n  purpose: Ensure do_generate fails when OpenAI response payload contains top-level response.error, matching TS provider behavior.\n  integration contract: Given a response with response.error, ai-sdk-rs must surface an error instead of returning normal content.\n- [ip-stream-failed-finish] Streaming response.failed finish semantics\n  purpose: Align response.failed/error stream terminal behavior with TS finish reason semantics.\n  integration contract: For error/failed stream trajectories, finish reason and terminal parts match TS parity expectations.\n- [ip-usage-details] Usage details normalization\n  purpose: Map nested usage detail fields (cached and reasoning token details) into normalized usage outputs.\n  integration contract: input_tokens_details.cached_tokens and output_tokens_details.reasoning_tokens are preserved in normalized usage fields.\n- [ip-function-tool-strict] Function tool strict passthrough\n  purpose: Preserve function tool strict mode when building Responses API request bodies.\n  integration contract: Function tools include strict=true/false when specified and omit strict when unspecified.\n\n### global constraints\n- Scope is existing provider only: openai responses path in ai-sdk-rs.\n- Use fixture-based tests; no live provider API calls.\n- Preserve normalized stream contract in crates/sdk-types/src/v2.rs and avoid introducing incompatible stream part shapes.\n- Keep fixes localized to OpenAI provider/core mapping paths unless type support requires shared sdk-types changes.\n\n### definition of done\n- All four parity gaps are covered by deterministic tests and passing implementations.\n- OpenAI provider targeted tests pass for both request serialization and stream normalization paths.\n- No dependency cycles exist in bead graph and execution order reflects blocker intent.\n- Final parity note lists resolved gaps and any residual risk with file-level references.\n\n### plan assumptions\n- Current TS behavior in /home/mike/ai/packages/openai/src/responses is the parity baseline.\n- OpenAI fixture corpus in crates/providers/openai/tests/fixtures is authoritative for offline validation.\n- Rust stream finish behavior should align with TS snapshots for response.failed/error trajectories.\n\n### plan risks\n- Adjusting finish semantics may affect downstream consumers that implicitly depended on previous finish reasons.\n- Adding strict support may require shared type updates that can affect other providers if not isolated.\n- Usage mapping changes could unintentionally alter reported token accounting in unrelated tests.\n\n## bead-local scope\nbead key: test-usage-nested-details-mapping\ntask: Add failing regression tests that assert mapping of input_tokens_details.cached_tokens and output_tokens_details.reasoning_tokens in non-stream and stream finish outputs.\nintegration points touched: ip-usage-details\nwhy: n/a\n\n### deliverables\n- Non-stream usage assertions in responses_language_model_tests.rs\n- Stream finish usage assertions in stream_fixture_tests.rs\n\n### acceptance\n- Tests fail on current behavior where nested details are not fully mapped\n- Tests are fixture-based and do not hit network\n\n### dependency intent\n- openai-parity-gap-matrix\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:2349549 picked-at=2026-02-14T02:13:40Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-62d.lock","status":"closed","priority":1,"issue_type":"task","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T18:02:05.237686513-08:00","created_by":"pomterre","updated_at":"2026-02-13T18:20:25.086942695-08:00","closed_at":"2026-02-13T18:20:25.086942695-08:00","close_reason":"Added failing regression assertions for nested usage detail mapping in non-stream generate and stream finish paths","labels":["openai","parity","tests","usage"],"dependencies":[{"issue_id":"ai-sdk-rs-62d","depends_on_id":"ai-sdk-rs-8uf","type":"blocks","created_at":"2026-02-13T18:02:43.923625414-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-650","title":"Align response.failed stream finish semantics","description":"# atomic bead\n\n## full plan snapshot\nplan title: OpenAI Responses parity gap closure (atomized)\nplan summary: Close the OpenAI Responses parity gaps against /home/mike/ai/packages/openai/src/responses by landing error-path, finish semantics, usage mapping, and function tool strict-mode parity with fixture-backed regressions.\n\n### integration points (complete map)\n- [ip-nonstream-response-error] Non-stream top-level error behavior\n  purpose: Ensure do_generate fails when response payload contains top-level error, matching TS behavior.\n  integration contract: If response.error exists, return SdkError::Upstream instead of GenerateResponse.\n- [ip-stream-failed-finish] Stream failed-finish semantics\n  purpose: Align response.failed stream trajectories with TS finish semantics and metadata invariants.\n  integration contract: response.failed must not be treated as response.completed/incomplete; finish reason remains Other with correct metadata behavior.\n- [ip-usage-details] Nested usage details mapping\n  purpose: Map cached and reasoning token details from nested provider usage structures.\n  integration contract: input_tokens_details.cached_tokens -\u003e Usage.cached_input_tokens and output_tokens_details.reasoning_tokens -\u003e Usage.reasoning_tokens.\n- [ip-function-tool-strict] Function tool strict passthrough\n  purpose: Preserve strict true/false/unset semantics in function tool request serialization.\n  integration contract: Serialize strict when explicitly true/false and omit when unset, matching TS prepare-tools behavior.\n\n### global constraints\n- Existing provider only: openai responses path in ai-sdk-rs; do not introduce new providers.\n- Prefer fixture-based tests; do not call live provider APIs.\n- Preserve normalized StreamPart/Event semantics and keep TS baseline behavior authoritative.\n- Keep changes scoped to openai provider and sdk-types only where required for strict parity.\n\n### definition of done\n- All beads reference at least one integration point and can execute independently with local context.\n- OpenAI parity regressions for the targeted behaviors are covered by deterministic tests.\n- cargo test --lib provider_openai:: passes for updated tests.\n- Dependency graph has no cycles and intended blocker relationships are encoded.\n\n### plan assumptions\n- docs/plans/openai-responses-parity-gap-matrix.md is the source plan for this atomization.\n- Fixture files under crates/providers/openai/tests/fixtures remain available and stable.\n- TS baseline files under /home/mike/ai/packages/openai/src/responses remain the parity reference.\n\n### plan risks\n- Finish-reason handling changes can regress stream ordering if hooks are altered incorrectly.\n- Usage mapping can silently drift if nested token fields are not tested in both stream and non-stream paths.\n- Function strict-mode parity can regress if provider options fallback and typed field behavior diverge.\n\n## bead-local scope\nbead key: stream-failed-finish-impl\ntask: Implement or preserve stream mapper behavior where response.failed is treated as terminal failure trajectory without completed/incomplete finish hint semantics.\nintegration points touched: ip-stream-failed-finish\nwhy: Finish reason and metadata invariants differ from normal completed/incomplete chunks and must match TS.\n\n### deliverables\n- Mapper/hook logic that keeps failed terminal trajectories on FinishReason::Other\n- No serviceTier propagation for failed-only terminal path where TS omits it\n\n### acceptance\n- Failed stream emits error part and finish part in expected order\n- Finish reason and metadata match TS baseline expectations\n\n### dependency intent\n- nonstream-error-impl-guard\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:2903394 picked-at=2026-02-14T04:08:11Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-650.lock","status":"closed","priority":0,"issue_type":"task","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T19:59:48.700928713-08:00","created_by":"pomterre","updated_at":"2026-02-13T20:09:20.345718005-08:00","closed_at":"2026-02-13T20:09:20.345718005-08:00","close_reason":"Enforced failed terminal stream parity by preventing serviceTier emission on response.failed trajectories while keeping finish reason Other","labels":["finish","openai","parity","stream"],"dependencies":[{"issue_id":"ai-sdk-rs-650","depends_on_id":"ai-sdk-rs-45c","type":"blocks","created_at":"2026-02-13T19:59:49.324320286-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-7jd","title":"Align response.failed stream finish semantics with TS","description":"# atomic bead\n\n## full plan snapshot\nplan title: OpenAI Responses parity remediation for ai-sdk-rs\nplan summary: Close behavior drift between ai-sdk-rs OpenAI Responses provider and /home/mike/ai by fixing non-stream error propagation, response.failed streaming finish semantics, usage detail mapping, and function tool strict passthrough, with fixture-based regression coverage.\n\n### integration points (complete map)\n- [ip-nonstream-response-error] Non-stream response.error propagation\n  purpose: Ensure do_generate fails when OpenAI response payload contains top-level response.error, matching TS provider behavior.\n  integration contract: Given a response with response.error, ai-sdk-rs must surface an error instead of returning normal content.\n- [ip-stream-failed-finish] Streaming response.failed finish semantics\n  purpose: Align response.failed/error stream terminal behavior with TS finish reason semantics.\n  integration contract: For error/failed stream trajectories, finish reason and terminal parts match TS parity expectations.\n- [ip-usage-details] Usage details normalization\n  purpose: Map nested usage detail fields (cached and reasoning token details) into normalized usage outputs.\n  integration contract: input_tokens_details.cached_tokens and output_tokens_details.reasoning_tokens are preserved in normalized usage fields.\n- [ip-function-tool-strict] Function tool strict passthrough\n  purpose: Preserve function tool strict mode when building Responses API request bodies.\n  integration contract: Function tools include strict=true/false when specified and omit strict when unspecified.\n\n### global constraints\n- Scope is existing provider only: openai responses path in ai-sdk-rs.\n- Use fixture-based tests; no live provider API calls.\n- Preserve normalized stream contract in crates/sdk-types/src/v2.rs and avoid introducing incompatible stream part shapes.\n- Keep fixes localized to OpenAI provider/core mapping paths unless type support requires shared sdk-types changes.\n\n### definition of done\n- All four parity gaps are covered by deterministic tests and passing implementations.\n- OpenAI provider targeted tests pass for both request serialization and stream normalization paths.\n- No dependency cycles exist in bead graph and execution order reflects blocker intent.\n- Final parity note lists resolved gaps and any residual risk with file-level references.\n\n### plan assumptions\n- Current TS behavior in /home/mike/ai/packages/openai/src/responses is the parity baseline.\n- OpenAI fixture corpus in crates/providers/openai/tests/fixtures is authoritative for offline validation.\n- Rust stream finish behavior should align with TS snapshots for response.failed/error trajectories.\n\n### plan risks\n- Adjusting finish semantics may affect downstream consumers that implicitly depended on previous finish reasons.\n- Adding strict support may require shared type updates that can affect other providers if not isolated.\n- Usage mapping changes could unintentionally alter reported token accounting in unrelated tests.\n\n## bead-local scope\nbead key: fix-stream-response-failed-finish\ntask: Adjust OpenAI Responses streaming parser/mapper so response.failed and error trajectories emit parity-aligned finish reason behavior.\nintegration points touched: ip-stream-failed-finish\nwhy: n/a\n\n### deliverables\n- Code updates in OpenAI stream chunk parsing and/or mapper finish logic\n- Updated comments explaining failed/error terminal semantics\n\n### acceptance\n- Stream response.failed regression test passes\n- Existing stream fixture suite continues to pass\n\n### dependency intent\n- test-stream-response-failed-finish\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:2416100 picked-at=2026-02-14T02:29:58Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-7jd.lock","status":"closed","priority":0,"issue_type":"bug","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T18:02:05.191999293-08:00","created_by":"pomterre","updated_at":"2026-02-13T18:56:05.640510657-08:00","closed_at":"2026-02-13T18:56:05.640510657-08:00","close_reason":"Validated response.failed stream finish semantics parity; openai stream fixture suite passes","labels":["bugfix","openai","parity","stream"],"dependencies":[{"issue_id":"ai-sdk-rs-7jd","depends_on_id":"ai-sdk-rs-9jf","type":"blocks","created_at":"2026-02-13T18:02:43.884111236-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-7td","title":"Implement nested usage detail parity mapping","description":"# atomic bead\n\n## full plan snapshot\nplan title: OpenAI Responses parity remediation for ai-sdk-rs\nplan summary: Close behavior drift between ai-sdk-rs OpenAI Responses provider and /home/mike/ai by fixing non-stream error propagation, response.failed streaming finish semantics, usage detail mapping, and function tool strict passthrough, with fixture-based regression coverage.\n\n### integration points (complete map)\n- [ip-nonstream-response-error] Non-stream response.error propagation\n  purpose: Ensure do_generate fails when OpenAI response payload contains top-level response.error, matching TS provider behavior.\n  integration contract: Given a response with response.error, ai-sdk-rs must surface an error instead of returning normal content.\n- [ip-stream-failed-finish] Streaming response.failed finish semantics\n  purpose: Align response.failed/error stream terminal behavior with TS finish reason semantics.\n  integration contract: For error/failed stream trajectories, finish reason and terminal parts match TS parity expectations.\n- [ip-usage-details] Usage details normalization\n  purpose: Map nested usage detail fields (cached and reasoning token details) into normalized usage outputs.\n  integration contract: input_tokens_details.cached_tokens and output_tokens_details.reasoning_tokens are preserved in normalized usage fields.\n- [ip-function-tool-strict] Function tool strict passthrough\n  purpose: Preserve function tool strict mode when building Responses API request bodies.\n  integration contract: Function tools include strict=true/false when specified and omit strict when unspecified.\n\n### global constraints\n- Scope is existing provider only: openai responses path in ai-sdk-rs.\n- Use fixture-based tests; no live provider API calls.\n- Preserve normalized stream contract in crates/sdk-types/src/v2.rs and avoid introducing incompatible stream part shapes.\n- Keep fixes localized to OpenAI provider/core mapping paths unless type support requires shared sdk-types changes.\n\n### definition of done\n- All four parity gaps are covered by deterministic tests and passing implementations.\n- OpenAI provider targeted tests pass for both request serialization and stream normalization paths.\n- No dependency cycles exist in bead graph and execution order reflects blocker intent.\n- Final parity note lists resolved gaps and any residual risk with file-level references.\n\n### plan assumptions\n- Current TS behavior in /home/mike/ai/packages/openai/src/responses is the parity baseline.\n- OpenAI fixture corpus in crates/providers/openai/tests/fixtures is authoritative for offline validation.\n- Rust stream finish behavior should align with TS snapshots for response.failed/error trajectories.\n\n### plan risks\n- Adjusting finish semantics may affect downstream consumers that implicitly depended on previous finish reasons.\n- Adding strict support may require shared type updates that can affect other providers if not isolated.\n- Usage mapping changes could unintentionally alter reported token accounting in unrelated tests.\n\n## bead-local scope\nbead key: fix-usage-nested-details-mapping\ntask: Update usage parsing/event mapping to preserve cached and reasoning token details in normalized usage outputs for generate and stream paths.\nintegration points touched: ip-usage-details\nwhy: n/a\n\n### deliverables\n- Code updates in OpenAI usage parsing and/or core event mapping\n- Reasoning token and cached token fields populated in normalized usage\n\n### acceptance\n- Nested usage detail regression tests pass\n- No unrelated provider usage tests regress\n\n### dependency intent\n- test-usage-nested-details-mapping\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:2424230 picked-at=2026-02-14T02:30:35Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-7td.lock","status":"closed","priority":0,"issue_type":"bug","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T18:02:05.288647675-08:00","created_by":"pomterre","updated_at":"2026-02-13T18:35:54.055415513-08:00","closed_at":"2026-02-13T18:35:54.055415513-08:00","close_reason":"Mapped nested input/output token detail fields into normalized usage for generate and stream paths; usage_maps_nested_details tests pass.","labels":["bugfix","openai","parity","usage"],"dependencies":[{"issue_id":"ai-sdk-rs-7td","depends_on_id":"ai-sdk-rs-62d","type":"blocks","created_at":"2026-02-13T18:02:43.972007109-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-8uf","title":"Codify OpenAI Responses parity gap matrix and fixture mapping","description":"# atomic bead\n\n## full plan snapshot\nplan title: OpenAI Responses parity remediation for ai-sdk-rs\nplan summary: Close behavior drift between ai-sdk-rs OpenAI Responses provider and /home/mike/ai by fixing non-stream error propagation, response.failed streaming finish semantics, usage detail mapping, and function tool strict passthrough, with fixture-based regression coverage.\n\n### integration points (complete map)\n- [ip-nonstream-response-error] Non-stream response.error propagation\n  purpose: Ensure do_generate fails when OpenAI response payload contains top-level response.error, matching TS provider behavior.\n  integration contract: Given a response with response.error, ai-sdk-rs must surface an error instead of returning normal content.\n- [ip-stream-failed-finish] Streaming response.failed finish semantics\n  purpose: Align response.failed/error stream terminal behavior with TS finish reason semantics.\n  integration contract: For error/failed stream trajectories, finish reason and terminal parts match TS parity expectations.\n- [ip-usage-details] Usage details normalization\n  purpose: Map nested usage detail fields (cached and reasoning token details) into normalized usage outputs.\n  integration contract: input_tokens_details.cached_tokens and output_tokens_details.reasoning_tokens are preserved in normalized usage fields.\n- [ip-function-tool-strict] Function tool strict passthrough\n  purpose: Preserve function tool strict mode when building Responses API request bodies.\n  integration contract: Function tools include strict=true/false when specified and omit strict when unspecified.\n\n### global constraints\n- Scope is existing provider only: openai responses path in ai-sdk-rs.\n- Use fixture-based tests; no live provider API calls.\n- Preserve normalized stream contract in crates/sdk-types/src/v2.rs and avoid introducing incompatible stream part shapes.\n- Keep fixes localized to OpenAI provider/core mapping paths unless type support requires shared sdk-types changes.\n\n### definition of done\n- All four parity gaps are covered by deterministic tests and passing implementations.\n- OpenAI provider targeted tests pass for both request serialization and stream normalization paths.\n- No dependency cycles exist in bead graph and execution order reflects blocker intent.\n- Final parity note lists resolved gaps and any residual risk with file-level references.\n\n### plan assumptions\n- Current TS behavior in /home/mike/ai/packages/openai/src/responses is the parity baseline.\n- OpenAI fixture corpus in crates/providers/openai/tests/fixtures is authoritative for offline validation.\n- Rust stream finish behavior should align with TS snapshots for response.failed/error trajectories.\n\n### plan risks\n- Adjusting finish semantics may affect downstream consumers that implicitly depended on previous finish reasons.\n- Adding strict support may require shared type updates that can affect other providers if not isolated.\n- Usage mapping changes could unintentionally alter reported token accounting in unrelated tests.\n\n## bead-local scope\nbead key: openai-parity-gap-matrix\ntask: Create a short matrix mapping each identified parity gap to target Rust files, TS baseline lines, and required fixtures/tests so subsequent beads can execute without external context.\nintegration points touched: ip-nonstream-response-error, ip-stream-failed-finish, ip-usage-details, ip-function-tool-strict\nwhy: Front-loading an explicit parity matrix prevents ambiguous implementation and preserves plan intent if context is lost.\n\n### deliverables\n- Gap-to-file mapping note with Rust and TS references\n- Fixture/test inventory for each gap\n\n### acceptance\n- Each integration point has explicit target files and expected behavior\n- Each downstream bead can be executed using only bead description\n\n### dependency intent\n- none\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:2322790 picked-at=2026-02-14T02:04:24Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-8uf.lock","status":"closed","priority":1,"issue_type":"task","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T18:02:04.999827916-08:00","created_by":"pomterre","updated_at":"2026-02-13T18:09:39.463114799-08:00","closed_at":"2026-02-13T18:09:39.463114799-08:00","close_reason":"Added OpenAI Responses parity gap matrix with Rust/TS references and fixture/test mapping in docs/plans/openai-responses-parity-gap-matrix.md","labels":["openai","parity","planning"]}
{"id":"ai-sdk-rs-9jf","title":"Add stream regression test for response.failed finish behavior","description":"# atomic bead\n\n## full plan snapshot\nplan title: OpenAI Responses parity remediation for ai-sdk-rs\nplan summary: Close behavior drift between ai-sdk-rs OpenAI Responses provider and /home/mike/ai by fixing non-stream error propagation, response.failed streaming finish semantics, usage detail mapping, and function tool strict passthrough, with fixture-based regression coverage.\n\n### integration points (complete map)\n- [ip-nonstream-response-error] Non-stream response.error propagation\n  purpose: Ensure do_generate fails when OpenAI response payload contains top-level response.error, matching TS provider behavior.\n  integration contract: Given a response with response.error, ai-sdk-rs must surface an error instead of returning normal content.\n- [ip-stream-failed-finish] Streaming response.failed finish semantics\n  purpose: Align response.failed/error stream terminal behavior with TS finish reason semantics.\n  integration contract: For error/failed stream trajectories, finish reason and terminal parts match TS parity expectations.\n- [ip-usage-details] Usage details normalization\n  purpose: Map nested usage detail fields (cached and reasoning token details) into normalized usage outputs.\n  integration contract: input_tokens_details.cached_tokens and output_tokens_details.reasoning_tokens are preserved in normalized usage fields.\n- [ip-function-tool-strict] Function tool strict passthrough\n  purpose: Preserve function tool strict mode when building Responses API request bodies.\n  integration contract: Function tools include strict=true/false when specified and omit strict when unspecified.\n\n### global constraints\n- Scope is existing provider only: openai responses path in ai-sdk-rs.\n- Use fixture-based tests; no live provider API calls.\n- Preserve normalized stream contract in crates/sdk-types/src/v2.rs and avoid introducing incompatible stream part shapes.\n- Keep fixes localized to OpenAI provider/core mapping paths unless type support requires shared sdk-types changes.\n\n### definition of done\n- All four parity gaps are covered by deterministic tests and passing implementations.\n- OpenAI provider targeted tests pass for both request serialization and stream normalization paths.\n- No dependency cycles exist in bead graph and execution order reflects blocker intent.\n- Final parity note lists resolved gaps and any residual risk with file-level references.\n\n### plan assumptions\n- Current TS behavior in /home/mike/ai/packages/openai/src/responses is the parity baseline.\n- OpenAI fixture corpus in crates/providers/openai/tests/fixtures is authoritative for offline validation.\n- Rust stream finish behavior should align with TS snapshots for response.failed/error trajectories.\n\n### plan risks\n- Adjusting finish semantics may affect downstream consumers that implicitly depended on previous finish reasons.\n- Adding strict support may require shared type updates that can affect other providers if not isolated.\n- Usage mapping changes could unintentionally alter reported token accounting in unrelated tests.\n\n## bead-local scope\nbead key: test-stream-response-failed-finish\ntask: Add a failing stream fixture regression that asserts finish reason/terminal parts for response.failed and error chunks match TS baseline expectations.\nintegration points touched: ip-stream-failed-finish\nwhy: Finish reason drift is subtle and must be pinned with explicit terminal-stream assertions.\n\n### deliverables\n- New assertions in crates/providers/openai/tests/stream_fixture_tests.rs\n- Validation of terminal finish reason and metadata for openai-error fixture\n\n### acceptance\n- Test fails against current response.failed handling\n- Expected outcome is explicitly documented in assertion messages\n\n### dependency intent\n- openai-parity-gap-matrix\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:2343495 picked-at=2026-02-14T02:11:37Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-9jf.lock","status":"closed","priority":1,"issue_type":"task","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T18:02:05.146023297-08:00","created_by":"pomterre","updated_at":"2026-02-13T18:18:34.789063853-08:00","closed_at":"2026-02-13T18:18:34.789063853-08:00","close_reason":"Added stream error regression assertions for terminal finish reason/order and provider metadata parity against TS baseline (openai-error.1).","labels":["openai","parity","stream","tests"],"dependencies":[{"issue_id":"ai-sdk-rs-9jf","depends_on_id":"ai-sdk-rs-8uf","type":"blocks","created_at":"2026-02-13T18:02:43.844327938-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-9v0","title":"Ensure function tool strict serialization parity","description":"# atomic bead\n\n## full plan snapshot\nplan title: OpenAI Responses parity gap closure (atomized)\nplan summary: Close the OpenAI Responses parity gaps against /home/mike/ai/packages/openai/src/responses by landing error-path, finish semantics, usage mapping, and function tool strict-mode parity with fixture-backed regressions.\n\n### integration points (complete map)\n- [ip-nonstream-response-error] Non-stream top-level error behavior\n  purpose: Ensure do_generate fails when response payload contains top-level error, matching TS behavior.\n  integration contract: If response.error exists, return SdkError::Upstream instead of GenerateResponse.\n- [ip-stream-failed-finish] Stream failed-finish semantics\n  purpose: Align response.failed stream trajectories with TS finish semantics and metadata invariants.\n  integration contract: response.failed must not be treated as response.completed/incomplete; finish reason remains Other with correct metadata behavior.\n- [ip-usage-details] Nested usage details mapping\n  purpose: Map cached and reasoning token details from nested provider usage structures.\n  integration contract: input_tokens_details.cached_tokens -\u003e Usage.cached_input_tokens and output_tokens_details.reasoning_tokens -\u003e Usage.reasoning_tokens.\n- [ip-function-tool-strict] Function tool strict passthrough\n  purpose: Preserve strict true/false/unset semantics in function tool request serialization.\n  integration contract: Serialize strict when explicitly true/false and omit when unset, matching TS prepare-tools behavior.\n\n### global constraints\n- Existing provider only: openai responses path in ai-sdk-rs; do not introduce new providers.\n- Prefer fixture-based tests; do not call live provider APIs.\n- Preserve normalized StreamPart/Event semantics and keep TS baseline behavior authoritative.\n- Keep changes scoped to openai provider and sdk-types only where required for strict parity.\n\n### definition of done\n- All beads reference at least one integration point and can execute independently with local context.\n- OpenAI parity regressions for the targeted behaviors are covered by deterministic tests.\n- cargo test --lib provider_openai:: passes for updated tests.\n- Dependency graph has no cycles and intended blocker relationships are encoded.\n\n### plan assumptions\n- docs/plans/openai-responses-parity-gap-matrix.md is the source plan for this atomization.\n- Fixture files under crates/providers/openai/tests/fixtures remain available and stable.\n- TS baseline files under /home/mike/ai/packages/openai/src/responses remain the parity reference.\n\n### plan risks\n- Finish-reason handling changes can regress stream ordering if hooks are altered incorrectly.\n- Usage mapping can silently drift if nested token fields are not tested in both stream and non-stream paths.\n- Function strict-mode parity can regress if provider options fallback and typed field behavior diverge.\n\n## bead-local scope\nbead key: function-tool-strict-serialization-impl\ntask: Implement or preserve strict passthrough in function tool serialization, including explicit true/false and omission when unset.\nintegration points touched: ip-function-tool-strict\nwhy: Strict mode controls JSON schema enforcement behavior and must match TS request shape exactly.\n\n### deliverables\n- FunctionTool strict field support in request serialization path\n- Fallback handling from provider options where applicable\n\n### acceptance\n- Request body includes strict:true/false only when explicitly set\n- Request body omits strict when unspecified\n\n### dependency intent\n- usage-details-regression-tests\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:2915504 picked-at=2026-02-14T04:12:40Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-9v0.lock","status":"closed","priority":1,"issue_type":"task","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T19:59:49.053044004-08:00","created_by":"pomterre","updated_at":"2026-02-13T20:13:37.227466237-08:00","closed_at":"2026-02-13T20:13:37.227466237-08:00","close_reason":"Validated strict serialization parity and added regression coverage for provider-option fallback and typed-field precedence","labels":["impl","openai","parity","strict","tools"],"dependencies":[{"issue_id":"ai-sdk-rs-9v0","depends_on_id":"ai-sdk-rs-dre","type":"blocks","created_at":"2026-02-13T19:59:49.61336213-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-9vu","title":"Extract shared provider bootstrap scaffold","description":"# atomic bead\n\n## full plan snapshot\nplan title: ai-sdk-rs fragility hardening and architecture LOC reduction\nplan summary: Harden known fragility points and reduce architecture duplication in provider stacks while preserving behavior parity and preventing coupling regression.\n\n### integration points (complete map)\n- [ip-openai-failure-semantics] OpenAI failure semantics\n  purpose: Ensure non-stream and stream failure paths return correct error/finish behavior.\n  integration contract: Non-stream top-level error must not return GenerateResponse success; stream failed trajectories must emit parity-aligned finish/error semantics.\n- [ip-transport-init-fallibility] Transport initialization fallibility\n  purpose: Prevent SDK process aborts from transport client init failures.\n  integration contract: Reqwest transport initialization surfaces recoverable errors instead of panicking.\n- [ip-google-stack-consolidation] Google and Vertex stack consolidation\n  purpose: Collapse duplicated provider implementations without behavior drift.\n  integration contract: Shared code preserves provider-specific auth, options scopes, and stream normalization semantics.\n- [ip-provider-bootstrap-dedupe] Provider bootstrap deduplication\n  purpose: Reduce repeated builder/header/default-option wiring across providers.\n  integration contract: Credential/env/header precedence and endpoint defaulting remain identical per provider.\n- [ip-error-mapping-unification] Error mapping unification\n  purpose: Unify fragmented error mapping with consistent diagnostics.\n  integration contract: Provider-specific parsing remains intact while fallback diagnostics become consistent and actionable.\n- [ip-module-boundary-normalization] Module boundary normalization\n  purpose: Reduce path-based boundary indirection and improve coupling observability.\n  integration contract: Public module exposure remains stable while internal structure improves analyzability and maintainability.\n\n### global constraints\n- No behavior or functionality loss.\n- Preserve public API contracts unless explicitly approved.\n- Keep provider request/response shapes and stream-part ordering stable.\n- Do not accept any coupling regression at the workspace gate.\n- Each slice must be validated with tests and quant_metrics snapshot/compare.\n\n### definition of done\n- OpenAI failure semantics match expected non-stream and stream parity behavior.\n- Transport initialization no longer panics on client build failure paths.\n- Google and Google Vertex duplicated stacks are consolidated behind shared modules with parity maintained.\n- Provider bootstrap and error-mapping duplication are reduced with equivalent behavior.\n- Path-based module boundary debt is reduced enough for coupling metrics to remain trustworthy for future slices.\n- quant_metrics compare verdict remains improves or no-regression for accepted slices.\n\n### plan assumptions\n- Current parity gap matrix for OpenAI responses reflects active known gaps.\n- inventory-based provider registration remains required.\n- Refactors can be staged incrementally without full rewrite.\n\n### plan risks\n- Provider parity regressions caused by shared-module extraction.\n- Hidden differences between Google GenAI and Google Vertex behavior during consolidation.\n- Coupling metric fidelity is partially limited by current path-based module structure.\n\n## bead-local scope\nbead key: provider-bootstrap-shared-scaffold\ntask: Introduce shared helper scaffolding for repeated provider builder flows (header filtering, defaults extraction, transport config wiring).\nintegration points touched: ip-provider-bootstrap-dedupe\nwhy: Reduces copy-paste fragility and makes future providers cheaper to add.\n\n### deliverables\n- Reusable bootstrap helpers\n- Refactored provider builders using common scaffold\n\n### acceptance\n- No behavior change in credential/env/header precedence\n- Provider registry build paths remain intact\n\n### dependency intent\n- google-vertex-shared-primitives\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:2528852 picked-at=2026-02-14T03:00:53Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-9vu.lock","status":"closed","priority":2,"issue_type":"feature","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T18:24:56.551099936-08:00","created_by":"pomterre","updated_at":"2026-02-13T19:21:10.768109018-08:00","closed_at":"2026-02-13T19:21:10.768109018-08:00","close_reason":"Shared provider bootstrap scaffold extracted across providers; tests passing.","labels":["architecture","loc-reduction","providers"],"dependencies":[{"issue_id":"ai-sdk-rs-9vu","depends_on_id":"ai-sdk-rs-bfg","type":"blocks","created_at":"2026-02-13T18:25:44.856024169-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-aaa","title":"Make Reqwest transport initialization fallible","description":"# atomic bead\n\n## full plan snapshot\nplan title: ai-sdk-rs fragility hardening and architecture LOC reduction\nplan summary: Harden known fragility points and reduce architecture duplication in provider stacks while preserving behavior parity and preventing coupling regression.\n\n### integration points (complete map)\n- [ip-openai-failure-semantics] OpenAI failure semantics\n  purpose: Ensure non-stream and stream failure paths return correct error/finish behavior.\n  integration contract: Non-stream top-level error must not return GenerateResponse success; stream failed trajectories must emit parity-aligned finish/error semantics.\n- [ip-transport-init-fallibility] Transport initialization fallibility\n  purpose: Prevent SDK process aborts from transport client init failures.\n  integration contract: Reqwest transport initialization surfaces recoverable errors instead of panicking.\n- [ip-google-stack-consolidation] Google and Vertex stack consolidation\n  purpose: Collapse duplicated provider implementations without behavior drift.\n  integration contract: Shared code preserves provider-specific auth, options scopes, and stream normalization semantics.\n- [ip-provider-bootstrap-dedupe] Provider bootstrap deduplication\n  purpose: Reduce repeated builder/header/default-option wiring across providers.\n  integration contract: Credential/env/header precedence and endpoint defaulting remain identical per provider.\n- [ip-error-mapping-unification] Error mapping unification\n  purpose: Unify fragmented error mapping with consistent diagnostics.\n  integration contract: Provider-specific parsing remains intact while fallback diagnostics become consistent and actionable.\n- [ip-module-boundary-normalization] Module boundary normalization\n  purpose: Reduce path-based boundary indirection and improve coupling observability.\n  integration contract: Public module exposure remains stable while internal structure improves analyzability and maintainability.\n\n### global constraints\n- No behavior or functionality loss.\n- Preserve public API contracts unless explicitly approved.\n- Keep provider request/response shapes and stream-part ordering stable.\n- Do not accept any coupling regression at the workspace gate.\n- Each slice must be validated with tests and quant_metrics snapshot/compare.\n\n### definition of done\n- OpenAI failure semantics match expected non-stream and stream parity behavior.\n- Transport initialization no longer panics on client build failure paths.\n- Google and Google Vertex duplicated stacks are consolidated behind shared modules with parity maintained.\n- Provider bootstrap and error-mapping duplication are reduced with equivalent behavior.\n- Path-based module boundary debt is reduced enough for coupling metrics to remain trustworthy for future slices.\n- quant_metrics compare verdict remains improves or no-regression for accepted slices.\n\n### plan assumptions\n- Current parity gap matrix for OpenAI responses reflects active known gaps.\n- inventory-based provider registration remains required.\n- Refactors can be staged incrementally without full rewrite.\n\n### plan risks\n- Provider parity regressions caused by shared-module extraction.\n- Hidden differences between Google GenAI and Google Vertex behavior during consolidation.\n- Coupling metric fidelity is partially limited by current path-based module structure.\n\n## bead-local scope\nbead key: reqwest-transport-fallible-init\ntask: Replace panic-based reqwest client build path with recoverable initialization error handling and propagate through call sites.\nintegration points touched: ip-transport-init-fallibility\nwhy: Avoids process aborts in SDK runtime setup paths.\n\n### deliverables\n- Fallible transport constructor path\n- Adjusted call sites or compatibility wrapper with explicit error behavior\n\n### acceptance\n- No panic on client-build failure path\n- Initialization failure surfaces as typed SDK/transport error\n\n### dependency intent\n- none\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:2415832 picked-at=2026-02-14T02:29:54Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-aaa.lock","status":"closed","priority":1,"issue_type":"task","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T18:24:56.084351939-08:00","created_by":"pomterre","updated_at":"2026-02-13T18:34:36.139782303-08:00","closed_at":"2026-02-13T18:34:36.139782303-08:00","close_reason":"Reqwest transport now exposes fallible init (try_new) and provider bootstrap call sites propagate initialization failures as SdkError::Transport instead of panicking.","labels":["fragility","transport"]}
{"id":"ai-sdk-rs-am3","title":"Capture baseline provider sdk matrix and dispatch gaps","description":"# atomic bead\n\n## full plan snapshot\nplan title: Provider Expansion Plan: Catalog-Normalized + ai-sdk-rs Registry Coverage\nplan summary: Normalize provider sdk labels at the Nodecode gateway appendix source and expand ai-sdk-rs provider routing so catalog providers resolve through existing implementations, starting with explicit Groq coverage and common OpenAI-compatible provider IDs.\n\n### integration points (complete map)\n- [ip-upstream-catalog-normalization] Gateway appendix sdk normalization\n  purpose: Guarantee emitted provider sdk values are canonical for ai-sdk-rs routing.\n  integration contract: appendix providers[].sdk values must be from the ai-sdk-rs-supported sdk set and preserve groq as groq.\n- [ip-openai-compatible-provider-id-routing] OpenAI-compatible provider-id routing in ai-sdk-rs\n  purpose: Resolve common provider IDs directly to OpenAI-compatible builder behavior.\n  integration contract: sdk_type_from_id and registry coverage must include target provider IDs with deterministic routing.\n- [ip-groq-routing] Explicit Groq routing support\n  purpose: Route sdk_type=groq catalog providers through a supported builder path.\n  integration contract: Groq provider definition must build a language model without fallback ambiguity.\n- [ip-nodecode-catalog-path] Nodecode catalog path precedence\n  purpose: Align runtime catalog reads to ~/.cache/nodecode/providers/catalog.json with safe fallback.\n  integration contract: path/env precedence is deterministic and test-covered for both nodecode and legacy paths.\n- [ip-fixture-parity-coverage] Fixture-based parity coverage for expanded provider IDs\n  purpose: Prevent request/stream/error mapping regressions while widening provider ID coverage.\n  integration contract: offline fixtures assert request serialization, stream normalization, and error mapping invariants.\n\n### global constraints\n- Preserve existing provider behavior for openai/anthropic/google/google-vertex/amazon-bedrock.\n- Use fixture-based parity tests only; no live provider API calls.\n- Keep Rust changes limited to currently representable interfaces: language, embeddings, images.\n- Any provider sdk label emitted by appendix must be parseable and routable by ai-sdk-rs without ad-hoc fallbacks.\n\n### definition of done\n- Gateway appendix export emits normalized sdk labels compatible with ai-sdk-rs.\n- ai-sdk-rs resolves and builds models for Groq and selected OpenAI-compatible provider IDs.\n- Capabilities loader prefers ~/.cache/nodecode/providers/catalog.json with env override precedence.\n- New regression tests cover sdk normalization, registry resolution, path precedence, and provider parity fixtures.\n- Dependency graph has no cycles and all created beads are ready/traceable.\n\n### plan assumptions\n- Catalog source of truth is generated in ~/nodecode-gateway-1 via modrepo appendix tooling.\n- Provider-specific TS surfaces without Rust interfaces (for example transcription/reranking) are explicitly deferred.\n- openai-compatible Rust implementation remains the shared execution path for aliased provider IDs.\n\n### plan risks\n- Unnormalized sdk labels can silently break provider dispatch at runtime.\n- Alias expansion may regress provider option scoping if provider_scope_name handling drifts.\n- Cache path changes can cause environment-specific behavior if precedence is not tested.\n\n## bead-local scope\nbead key: provider-expansion-baseline-matrix\ntask: Document current sdk labels from ~/.cache/nodecode/providers/catalog.json and map unresolved ai-sdk-rs dispatch cases before any code changes.\nintegration points touched: ip-upstream-catalog-normalization, ip-openai-compatible-provider-id-routing, ip-groq-routing\nwhy: A concrete baseline prevents accidental over-scope and anchors implementation acceptance.\n\n### deliverables\n- matrix of provider ids to sdk labels from active nodecode catalog\n- list of unresolved dispatch gaps with file-level targets\n\n### acceptance\n- baseline artifact clearly identifies Groq and OpenAI-compatible alias coverage deltas\n- integration points are explicitly mapped to implementation files\n\n### dependency intent\n- none\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:3021244 picked-at=2026-02-14T04:35:38Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-am3.lock","status":"closed","priority":0,"issue_type":"task","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T20:33:08.766386761-08:00","created_by":"pomterre","updated_at":"2026-02-13T20:42:42.974685307-08:00","closed_at":"2026-02-13T20:42:42.974685307-08:00","close_reason":"Added baseline provider-id to sdk_type matrix from live nodecode catalog and mapped unresolved OpenAI-compatible/Groq dispatch gaps to concrete target files.","labels":["planning","provider-parity"]}
{"id":"ai-sdk-rs-bfg","title":"Extract shared Google/Vertex primitives","description":"# atomic bead\n\n## full plan snapshot\nplan title: ai-sdk-rs fragility hardening and architecture LOC reduction\nplan summary: Harden known fragility points and reduce architecture duplication in provider stacks while preserving behavior parity and preventing coupling regression.\n\n### integration points (complete map)\n- [ip-openai-failure-semantics] OpenAI failure semantics\n  purpose: Ensure non-stream and stream failure paths return correct error/finish behavior.\n  integration contract: Non-stream top-level error must not return GenerateResponse success; stream failed trajectories must emit parity-aligned finish/error semantics.\n- [ip-transport-init-fallibility] Transport initialization fallibility\n  purpose: Prevent SDK process aborts from transport client init failures.\n  integration contract: Reqwest transport initialization surfaces recoverable errors instead of panicking.\n- [ip-google-stack-consolidation] Google and Vertex stack consolidation\n  purpose: Collapse duplicated provider implementations without behavior drift.\n  integration contract: Shared code preserves provider-specific auth, options scopes, and stream normalization semantics.\n- [ip-provider-bootstrap-dedupe] Provider bootstrap deduplication\n  purpose: Reduce repeated builder/header/default-option wiring across providers.\n  integration contract: Credential/env/header precedence and endpoint defaulting remain identical per provider.\n- [ip-error-mapping-unification] Error mapping unification\n  purpose: Unify fragmented error mapping with consistent diagnostics.\n  integration contract: Provider-specific parsing remains intact while fallback diagnostics become consistent and actionable.\n- [ip-module-boundary-normalization] Module boundary normalization\n  purpose: Reduce path-based boundary indirection and improve coupling observability.\n  integration contract: Public module exposure remains stable while internal structure improves analyzability and maintainability.\n\n### global constraints\n- No behavior or functionality loss.\n- Preserve public API contracts unless explicitly approved.\n- Keep provider request/response shapes and stream-part ordering stable.\n- Do not accept any coupling regression at the workspace gate.\n- Each slice must be validated with tests and quant_metrics snapshot/compare.\n\n### definition of done\n- OpenAI failure semantics match expected non-stream and stream parity behavior.\n- Transport initialization no longer panics on client build failure paths.\n- Google and Google Vertex duplicated stacks are consolidated behind shared modules with parity maintained.\n- Provider bootstrap and error-mapping duplication are reduced with equivalent behavior.\n- Path-based module boundary debt is reduced enough for coupling metrics to remain trustworthy for future slices.\n- quant_metrics compare verdict remains improves or no-regression for accepted slices.\n\n### plan assumptions\n- Current parity gap matrix for OpenAI responses reflects active known gaps.\n- inventory-based provider registration remains required.\n- Refactors can be staged incrementally without full rewrite.\n\n### plan risks\n- Provider parity regressions caused by shared-module extraction.\n- Hidden differences between Google GenAI and Google Vertex behavior during consolidation.\n- Coupling metric fidelity is partially limited by current path-based module structure.\n\n## bead-local scope\nbead key: google-vertex-shared-primitives\ntask: Consolidate duplicated error/options/prompt/prepare_tools modules into shared primitives with provider-specific adapters.\nintegration points touched: ip-google-stack-consolidation\nwhy: Largest low-risk duplicate surface for immediate LOC reduction.\n\n### deliverables\n- Shared primitives for prompt/options/tool-prep/error mapping\n- Provider adapters preserving google vs google-vertex scope differences\n\n### acceptance\n- Duplicate files are removed or reduced significantly\n- Provider behavior remains parity-equivalent in tests\n\n### dependency intent\n- none\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:2479910 picked-at=2026-02-14T02:44:07Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-bfg.lock","status":"closed","priority":1,"issue_type":"feature","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T18:24:56.321291139-08:00","created_by":"pomterre","updated_at":"2026-02-13T18:55:12.021619336-08:00","closed_at":"2026-02-13T18:55:12.021619336-08:00","close_reason":"Extracted shared Google/Vertex primitives for error mapping, provider options parsing, prompt conversion, and tool preparation with provider-scope adapters; validated via cargo test.","labels":["architecture","google","loc-reduction"]}
{"id":"ai-sdk-rs-bxp","title":"Align capabilities catalog default path to nodecode cache","description":"# atomic bead\n\n## full plan snapshot\nplan title: Provider Expansion Plan: Catalog-Normalized + ai-sdk-rs Registry Coverage\nplan summary: Normalize provider sdk labels at the Nodecode gateway appendix source and expand ai-sdk-rs provider routing so catalog providers resolve through existing implementations, starting with explicit Groq coverage and common OpenAI-compatible provider IDs.\n\n### integration points (complete map)\n- [ip-upstream-catalog-normalization] Gateway appendix sdk normalization\n  purpose: Guarantee emitted provider sdk values are canonical for ai-sdk-rs routing.\n  integration contract: appendix providers[].sdk values must be from the ai-sdk-rs-supported sdk set and preserve groq as groq.\n- [ip-openai-compatible-provider-id-routing] OpenAI-compatible provider-id routing in ai-sdk-rs\n  purpose: Resolve common provider IDs directly to OpenAI-compatible builder behavior.\n  integration contract: sdk_type_from_id and registry coverage must include target provider IDs with deterministic routing.\n- [ip-groq-routing] Explicit Groq routing support\n  purpose: Route sdk_type=groq catalog providers through a supported builder path.\n  integration contract: Groq provider definition must build a language model without fallback ambiguity.\n- [ip-nodecode-catalog-path] Nodecode catalog path precedence\n  purpose: Align runtime catalog reads to ~/.cache/nodecode/providers/catalog.json with safe fallback.\n  integration contract: path/env precedence is deterministic and test-covered for both nodecode and legacy paths.\n- [ip-fixture-parity-coverage] Fixture-based parity coverage for expanded provider IDs\n  purpose: Prevent request/stream/error mapping regressions while widening provider ID coverage.\n  integration contract: offline fixtures assert request serialization, stream normalization, and error mapping invariants.\n\n### global constraints\n- Preserve existing provider behavior for openai/anthropic/google/google-vertex/amazon-bedrock.\n- Use fixture-based parity tests only; no live provider API calls.\n- Keep Rust changes limited to currently representable interfaces: language, embeddings, images.\n- Any provider sdk label emitted by appendix must be parseable and routable by ai-sdk-rs without ad-hoc fallbacks.\n\n### definition of done\n- Gateway appendix export emits normalized sdk labels compatible with ai-sdk-rs.\n- ai-sdk-rs resolves and builds models for Groq and selected OpenAI-compatible provider IDs.\n- Capabilities loader prefers ~/.cache/nodecode/providers/catalog.json with env override precedence.\n- New regression tests cover sdk normalization, registry resolution, path precedence, and provider parity fixtures.\n- Dependency graph has no cycles and all created beads are ready/traceable.\n\n### plan assumptions\n- Catalog source of truth is generated in ~/nodecode-gateway-1 via modrepo appendix tooling.\n- Provider-specific TS surfaces without Rust interfaces (for example transcription/reranking) are explicitly deferred.\n- openai-compatible Rust implementation remains the shared execution path for aliased provider IDs.\n\n### plan risks\n- Unnormalized sdk labels can silently break provider dispatch at runtime.\n- Alias expansion may regress provider option scoping if provider_scope_name handling drifts.\n- Cache path changes can cause environment-specific behavior if precedence is not tested.\n\n## bead-local scope\nbead key: ai-sdk-rs-nodecode-catalog-path-order\ntask: Update crates/core/src/capabilities.rs to prefer ~/.cache/nodecode/providers/catalog.json after env overrides, retaining legacy fallback.\nintegration points touched: ip-nodecode-catalog-path\nwhy: Runtime capability lookups must read the active cache location used by nodecode.\n\n### deliverables\n- default path lookup order update\n- comments clarifying precedence and fallback policy\n\n### acceptance\n- nodecode cache path is first default on disk lookup\n- legacy clixode path remains available as fallback\n\n### dependency intent\n- provider-expansion-baseline-matrix\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:3086641 picked-at=2026-02-14T04:51:46Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-bxp.lock","status":"in_progress","priority":1,"issue_type":"feature","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T20:33:09.118083269-08:00","created_by":"pomterre","updated_at":"2026-02-13T20:51:46.987555411-08:00","labels":["ai-sdk-rs","capabilities"],"dependencies":[{"issue_id":"ai-sdk-rs-bxp","depends_on_id":"ai-sdk-rs-am3","type":"blocks","created_at":"2026-02-13T20:33:09.604124689-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-cg7","title":"Add capabilities path precedence and shape compatibility tests","description":"# atomic bead\n\n## full plan snapshot\nplan title: Provider Expansion Plan: Catalog-Normalized + ai-sdk-rs Registry Coverage\nplan summary: Normalize provider sdk labels at the Nodecode gateway appendix source and expand ai-sdk-rs provider routing so catalog providers resolve through existing implementations, starting with explicit Groq coverage and common OpenAI-compatible provider IDs.\n\n### integration points (complete map)\n- [ip-upstream-catalog-normalization] Gateway appendix sdk normalization\n  purpose: Guarantee emitted provider sdk values are canonical for ai-sdk-rs routing.\n  integration contract: appendix providers[].sdk values must be from the ai-sdk-rs-supported sdk set and preserve groq as groq.\n- [ip-openai-compatible-provider-id-routing] OpenAI-compatible provider-id routing in ai-sdk-rs\n  purpose: Resolve common provider IDs directly to OpenAI-compatible builder behavior.\n  integration contract: sdk_type_from_id and registry coverage must include target provider IDs with deterministic routing.\n- [ip-groq-routing] Explicit Groq routing support\n  purpose: Route sdk_type=groq catalog providers through a supported builder path.\n  integration contract: Groq provider definition must build a language model without fallback ambiguity.\n- [ip-nodecode-catalog-path] Nodecode catalog path precedence\n  purpose: Align runtime catalog reads to ~/.cache/nodecode/providers/catalog.json with safe fallback.\n  integration contract: path/env precedence is deterministic and test-covered for both nodecode and legacy paths.\n- [ip-fixture-parity-coverage] Fixture-based parity coverage for expanded provider IDs\n  purpose: Prevent request/stream/error mapping regressions while widening provider ID coverage.\n  integration contract: offline fixtures assert request serialization, stream normalization, and error mapping invariants.\n\n### global constraints\n- Preserve existing provider behavior for openai/anthropic/google/google-vertex/amazon-bedrock.\n- Use fixture-based parity tests only; no live provider API calls.\n- Keep Rust changes limited to currently representable interfaces: language, embeddings, images.\n- Any provider sdk label emitted by appendix must be parseable and routable by ai-sdk-rs without ad-hoc fallbacks.\n\n### definition of done\n- Gateway appendix export emits normalized sdk labels compatible with ai-sdk-rs.\n- ai-sdk-rs resolves and builds models for Groq and selected OpenAI-compatible provider IDs.\n- Capabilities loader prefers ~/.cache/nodecode/providers/catalog.json with env override precedence.\n- New regression tests cover sdk normalization, registry resolution, path precedence, and provider parity fixtures.\n- Dependency graph has no cycles and all created beads are ready/traceable.\n\n### plan assumptions\n- Catalog source of truth is generated in ~/nodecode-gateway-1 via modrepo appendix tooling.\n- Provider-specific TS surfaces without Rust interfaces (for example transcription/reranking) are explicitly deferred.\n- openai-compatible Rust implementation remains the shared execution path for aliased provider IDs.\n\n### plan risks\n- Unnormalized sdk labels can silently break provider dispatch at runtime.\n- Alias expansion may regress provider option scoping if provider_scope_name handling drifts.\n- Cache path changes can cause environment-specific behavior if precedence is not tested.\n\n## bead-local scope\nbead key: ai-sdk-rs-catalog-path-precedence-tests\ntask: Add tests proving env override precedence and compatibility with nodecode catalog structure while preserving legacy fallback behavior.\nintegration points touched: ip-nodecode-catalog-path\nwhy: Path/order regressions are environment-sensitive and easy to miss without focused tests.\n\n### deliverables\n- unit tests for path precedence\n- tests for supported catalog JSON shape(s)\n\n### acceptance\n- tests validate env \u003e explicit path \u003e nodecode default \u003e legacy fallback\n- tests fail if nodecode catalog becomes unreadable\n\n### dependency intent\n- ai-sdk-rs-nodecode-catalog-path-order\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","status":"open","priority":2,"issue_type":"task","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T20:33:09.167682919-08:00","created_by":"pomterre","updated_at":"2026-02-13T20:33:09.167682919-08:00","labels":["ai-sdk-rs","capabilities","tests"],"dependencies":[{"issue_id":"ai-sdk-rs-cg7","depends_on_id":"ai-sdk-rs-bxp","type":"blocks","created_at":"2026-02-13T20:33:09.655325526-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-cpi","title":"Add non-stream top-level error regression contract","description":"# atomic bead\n\n## full plan snapshot\nplan title: OpenAI Responses parity gap closure (atomized)\nplan summary: Close the OpenAI Responses parity gaps against /home/mike/ai/packages/openai/src/responses by landing error-path, finish semantics, usage mapping, and function tool strict-mode parity with fixture-backed regressions.\n\n### integration points (complete map)\n- [ip-nonstream-response-error] Non-stream top-level error behavior\n  purpose: Ensure do_generate fails when response payload contains top-level error, matching TS behavior.\n  integration contract: If response.error exists, return SdkError::Upstream instead of GenerateResponse.\n- [ip-stream-failed-finish] Stream failed-finish semantics\n  purpose: Align response.failed stream trajectories with TS finish semantics and metadata invariants.\n  integration contract: response.failed must not be treated as response.completed/incomplete; finish reason remains Other with correct metadata behavior.\n- [ip-usage-details] Nested usage details mapping\n  purpose: Map cached and reasoning token details from nested provider usage structures.\n  integration contract: input_tokens_details.cached_tokens -\u003e Usage.cached_input_tokens and output_tokens_details.reasoning_tokens -\u003e Usage.reasoning_tokens.\n- [ip-function-tool-strict] Function tool strict passthrough\n  purpose: Preserve strict true/false/unset semantics in function tool request serialization.\n  integration contract: Serialize strict when explicitly true/false and omit when unset, matching TS prepare-tools behavior.\n\n### global constraints\n- Existing provider only: openai responses path in ai-sdk-rs; do not introduce new providers.\n- Prefer fixture-based tests; do not call live provider APIs.\n- Preserve normalized StreamPart/Event semantics and keep TS baseline behavior authoritative.\n- Keep changes scoped to openai provider and sdk-types only where required for strict parity.\n\n### definition of done\n- All beads reference at least one integration point and can execute independently with local context.\n- OpenAI parity regressions for the targeted behaviors are covered by deterministic tests.\n- cargo test --lib provider_openai:: passes for updated tests.\n- Dependency graph has no cycles and intended blocker relationships are encoded.\n\n### plan assumptions\n- docs/plans/openai-responses-parity-gap-matrix.md is the source plan for this atomization.\n- Fixture files under crates/providers/openai/tests/fixtures remain available and stable.\n- TS baseline files under /home/mike/ai/packages/openai/src/responses remain the parity reference.\n\n### plan risks\n- Finish-reason handling changes can regress stream ordering if hooks are altered incorrectly.\n- Usage mapping can silently drift if nested token fields are not tested in both stream and non-stream paths.\n- Function strict-mode parity can regress if provider options fallback and typed field behavior diverge.\n\n## bead-local scope\nbead key: nonstream-error-regression-contract\ntask: Add or tighten regression assertions proving do_generate returns an error for top-level response.error using openai-error fixture.\nintegration points touched: ip-nonstream-response-error\nwhy: Locks parity contract before implementation adjustments and prevents silent success-path regressions.\n\n### deliverables\n- Regression test covering openai-error.1 non-stream response path\n- Assertion on error type/message parity expectations\n\n### acceptance\n- Test fails without parity behavior and passes with correct error handling\n- No live API calls required\n\n### dependency intent\n- none\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:2859096 picked-at=2026-02-14T04:04:57Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-cpi.lock","status":"closed","priority":0,"issue_type":"task","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T19:59:48.519393001-08:00","created_by":"pomterre","updated_at":"2026-02-13T20:06:20.415333789-08:00","closed_at":"2026-02-13T20:06:20.415333789-08:00","close_reason":"Tightened non-stream response.error regression to assert exact OpenAI fixture message and upstream status/source invariants","labels":["nonstream","openai","parity","tests"]}
{"id":"ai-sdk-rs-d15","title":"Run end-to-end verification and document rollout checks","description":"# atomic bead\n\n## full plan snapshot\nplan title: Provider Expansion Plan: Catalog-Normalized + ai-sdk-rs Registry Coverage\nplan summary: Normalize provider sdk labels at the Nodecode gateway appendix source and expand ai-sdk-rs provider routing so catalog providers resolve through existing implementations, starting with explicit Groq coverage and common OpenAI-compatible provider IDs.\n\n### integration points (complete map)\n- [ip-upstream-catalog-normalization] Gateway appendix sdk normalization\n  purpose: Guarantee emitted provider sdk values are canonical for ai-sdk-rs routing.\n  integration contract: appendix providers[].sdk values must be from the ai-sdk-rs-supported sdk set and preserve groq as groq.\n- [ip-openai-compatible-provider-id-routing] OpenAI-compatible provider-id routing in ai-sdk-rs\n  purpose: Resolve common provider IDs directly to OpenAI-compatible builder behavior.\n  integration contract: sdk_type_from_id and registry coverage must include target provider IDs with deterministic routing.\n- [ip-groq-routing] Explicit Groq routing support\n  purpose: Route sdk_type=groq catalog providers through a supported builder path.\n  integration contract: Groq provider definition must build a language model without fallback ambiguity.\n- [ip-nodecode-catalog-path] Nodecode catalog path precedence\n  purpose: Align runtime catalog reads to ~/.cache/nodecode/providers/catalog.json with safe fallback.\n  integration contract: path/env precedence is deterministic and test-covered for both nodecode and legacy paths.\n- [ip-fixture-parity-coverage] Fixture-based parity coverage for expanded provider IDs\n  purpose: Prevent request/stream/error mapping regressions while widening provider ID coverage.\n  integration contract: offline fixtures assert request serialization, stream normalization, and error mapping invariants.\n\n### global constraints\n- Preserve existing provider behavior for openai/anthropic/google/google-vertex/amazon-bedrock.\n- Use fixture-based parity tests only; no live provider API calls.\n- Keep Rust changes limited to currently representable interfaces: language, embeddings, images.\n- Any provider sdk label emitted by appendix must be parseable and routable by ai-sdk-rs without ad-hoc fallbacks.\n\n### definition of done\n- Gateway appendix export emits normalized sdk labels compatible with ai-sdk-rs.\n- ai-sdk-rs resolves and builds models for Groq and selected OpenAI-compatible provider IDs.\n- Capabilities loader prefers ~/.cache/nodecode/providers/catalog.json with env override precedence.\n- New regression tests cover sdk normalization, registry resolution, path precedence, and provider parity fixtures.\n- Dependency graph has no cycles and all created beads are ready/traceable.\n\n### plan assumptions\n- Catalog source of truth is generated in ~/nodecode-gateway-1 via modrepo appendix tooling.\n- Provider-specific TS surfaces without Rust interfaces (for example transcription/reranking) are explicitly deferred.\n- openai-compatible Rust implementation remains the shared execution path for aliased provider IDs.\n\n### plan risks\n- Unnormalized sdk labels can silently break provider dispatch at runtime.\n- Alias expansion may regress provider option scoping if provider_scope_name handling drifts.\n- Cache path changes can cause environment-specific behavior if precedence is not tested.\n\n## bead-local scope\nbead key: provider-expansion-integration-verification\ntask: Execute targeted test suites and dependency checks, then document rollout verification steps for catalog generation and ai-sdk-rs routing acceptance.\nintegration points touched: ip-upstream-catalog-normalization, ip-openai-compatible-provider-id-routing, ip-groq-routing, ip-nodecode-catalog-path, ip-fixture-parity-coverage\nwhy: Cross-repo changes require a single verification checkpoint before implementation is considered complete.\n\n### deliverables\n- verification log with command outputs summarized\n- final acceptance checklist tied to all integration points\n\n### acceptance\n- all new tests pass in touched areas\n- no dependency cycles in bead graph\n- verification notes identify deferred TS-only surfaces explicitly\n\n### dependency intent\n- gateway-appendix-sdk-normalization-tests\n- ai-sdk-rs-registry-routing-tests\n- ai-sdk-rs-catalog-path-precedence-tests\n- ai-sdk-rs-openai-compatible-parity-fixtures\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","status":"open","priority":1,"issue_type":"task","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T20:33:09.266465801-08:00","created_by":"pomterre","updated_at":"2026-02-13T20:33:09.266465801-08:00","labels":["provider-parity","verification"],"dependencies":[{"issue_id":"ai-sdk-rs-d15","depends_on_id":"ai-sdk-rs-486","type":"blocks","created_at":"2026-02-13T20:33:09.773804206-08:00","created_by":"pomterre"},{"issue_id":"ai-sdk-rs-d15","depends_on_id":"ai-sdk-rs-3v9","type":"blocks","created_at":"2026-02-13T20:33:09.825757925-08:00","created_by":"pomterre"},{"issue_id":"ai-sdk-rs-d15","depends_on_id":"ai-sdk-rs-cg7","type":"blocks","created_at":"2026-02-13T20:33:09.890774056-08:00","created_by":"pomterre"},{"issue_id":"ai-sdk-rs-d15","depends_on_id":"ai-sdk-rs-r9n","type":"blocks","created_at":"2026-02-13T20:33:09.948753378-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-dre","title":"Add nested usage regression tests for stream and non-stream","description":"# atomic bead\n\n## full plan snapshot\nplan title: OpenAI Responses parity gap closure (atomized)\nplan summary: Close the OpenAI Responses parity gaps against /home/mike/ai/packages/openai/src/responses by landing error-path, finish semantics, usage mapping, and function tool strict-mode parity with fixture-backed regressions.\n\n### integration points (complete map)\n- [ip-nonstream-response-error] Non-stream top-level error behavior\n  purpose: Ensure do_generate fails when response payload contains top-level error, matching TS behavior.\n  integration contract: If response.error exists, return SdkError::Upstream instead of GenerateResponse.\n- [ip-stream-failed-finish] Stream failed-finish semantics\n  purpose: Align response.failed stream trajectories with TS finish semantics and metadata invariants.\n  integration contract: response.failed must not be treated as response.completed/incomplete; finish reason remains Other with correct metadata behavior.\n- [ip-usage-details] Nested usage details mapping\n  purpose: Map cached and reasoning token details from nested provider usage structures.\n  integration contract: input_tokens_details.cached_tokens -\u003e Usage.cached_input_tokens and output_tokens_details.reasoning_tokens -\u003e Usage.reasoning_tokens.\n- [ip-function-tool-strict] Function tool strict passthrough\n  purpose: Preserve strict true/false/unset semantics in function tool request serialization.\n  integration contract: Serialize strict when explicitly true/false and omit when unset, matching TS prepare-tools behavior.\n\n### global constraints\n- Existing provider only: openai responses path in ai-sdk-rs; do not introduce new providers.\n- Prefer fixture-based tests; do not call live provider APIs.\n- Preserve normalized StreamPart/Event semantics and keep TS baseline behavior authoritative.\n- Keep changes scoped to openai provider and sdk-types only where required for strict parity.\n\n### definition of done\n- All beads reference at least one integration point and can execute independently with local context.\n- OpenAI parity regressions for the targeted behaviors are covered by deterministic tests.\n- cargo test --lib provider_openai:: passes for updated tests.\n- Dependency graph has no cycles and intended blocker relationships are encoded.\n\n### plan assumptions\n- docs/plans/openai-responses-parity-gap-matrix.md is the source plan for this atomization.\n- Fixture files under crates/providers/openai/tests/fixtures remain available and stable.\n- TS baseline files under /home/mike/ai/packages/openai/src/responses remain the parity reference.\n\n### plan risks\n- Finish-reason handling changes can regress stream ordering if hooks are altered incorrectly.\n- Usage mapping can silently drift if nested token fields are not tested in both stream and non-stream paths.\n- Function strict-mode parity can regress if provider options fallback and typed field behavior diverge.\n\n## bead-local scope\nbead key: usage-details-regression-tests\ntask: Add fixture-backed regressions asserting cached_input_tokens and reasoning_tokens in both do_generate and do_stream outputs.\nintegration points touched: ip-usage-details\nwhy: Guarantees parity for usage details across both execution modes.\n\n### deliverables\n- Non-stream usage-details regression test\n- Stream usage-details regression test\n\n### acceptance\n- Assertions validate nested detail mapping from existing fixtures\n- Tests run offline and deterministically\n\n### dependency intent\n- usage-details-mapping-impl\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:2912198 picked-at=2026-02-14T04:11:48Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-dre.lock","status":"closed","priority":1,"issue_type":"task","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T19:59:48.963943202-08:00","created_by":"pomterre","updated_at":"2026-02-13T20:12:24.264712509-08:00","closed_at":"2026-02-13T20:12:24.264712509-08:00","close_reason":"Verified non-stream and stream nested usage regression tests for cached_input_tokens and reasoning_tokens are present and passing","labels":["openai","parity","tests","usage"],"dependencies":[{"issue_id":"ai-sdk-rs-dre","depends_on_id":"ai-sdk-rs-ika","type":"blocks","created_at":"2026-02-13T19:59:49.546246448-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-efl","title":"Add function strict true/false/unset regression tests","description":"# atomic bead\n\n## full plan snapshot\nplan title: OpenAI Responses parity gap closure (atomized)\nplan summary: Close the OpenAI Responses parity gaps against /home/mike/ai/packages/openai/src/responses by landing error-path, finish semantics, usage mapping, and function tool strict-mode parity with fixture-backed regressions.\n\n### integration points (complete map)\n- [ip-nonstream-response-error] Non-stream top-level error behavior\n  purpose: Ensure do_generate fails when response payload contains top-level error, matching TS behavior.\n  integration contract: If response.error exists, return SdkError::Upstream instead of GenerateResponse.\n- [ip-stream-failed-finish] Stream failed-finish semantics\n  purpose: Align response.failed stream trajectories with TS finish semantics and metadata invariants.\n  integration contract: response.failed must not be treated as response.completed/incomplete; finish reason remains Other with correct metadata behavior.\n- [ip-usage-details] Nested usage details mapping\n  purpose: Map cached and reasoning token details from nested provider usage structures.\n  integration contract: input_tokens_details.cached_tokens -\u003e Usage.cached_input_tokens and output_tokens_details.reasoning_tokens -\u003e Usage.reasoning_tokens.\n- [ip-function-tool-strict] Function tool strict passthrough\n  purpose: Preserve strict true/false/unset semantics in function tool request serialization.\n  integration contract: Serialize strict when explicitly true/false and omit when unset, matching TS prepare-tools behavior.\n\n### global constraints\n- Existing provider only: openai responses path in ai-sdk-rs; do not introduce new providers.\n- Prefer fixture-based tests; do not call live provider APIs.\n- Preserve normalized StreamPart/Event semantics and keep TS baseline behavior authoritative.\n- Keep changes scoped to openai provider and sdk-types only where required for strict parity.\n\n### definition of done\n- All beads reference at least one integration point and can execute independently with local context.\n- OpenAI parity regressions for the targeted behaviors are covered by deterministic tests.\n- cargo test --lib provider_openai:: passes for updated tests.\n- Dependency graph has no cycles and intended blocker relationships are encoded.\n\n### plan assumptions\n- docs/plans/openai-responses-parity-gap-matrix.md is the source plan for this atomization.\n- Fixture files under crates/providers/openai/tests/fixtures remain available and stable.\n- TS baseline files under /home/mike/ai/packages/openai/src/responses remain the parity reference.\n\n### plan risks\n- Finish-reason handling changes can regress stream ordering if hooks are altered incorrectly.\n- Usage mapping can silently drift if nested token fields are not tested in both stream and non-stream paths.\n- Function strict-mode parity can regress if provider options fallback and typed field behavior diverge.\n\n## bead-local scope\nbead key: function-tool-strict-regression-tests\ntask: Add dedicated request-body regressions for strict true, strict false, and strict omitted cases.\nintegration points touched: ip-function-tool-strict\nwhy: Locks exact strict serialization contract to prevent future request-shape drift.\n\n### deliverables\n- Three strict serialization assertions (true/false/omitted)\n- Fixture or inline expected payload verification\n\n### acceptance\n- All strict-mode cases are explicitly asserted\n- No false positives from default-value serialization\n\n### dependency intent\n- function-tool-strict-serialization-impl\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:2921417 picked-at=2026-02-14T04:13:52Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-efl.lock","status":"closed","priority":1,"issue_type":"task","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T19:59:49.155507212-08:00","created_by":"pomterre","updated_at":"2026-02-13T20:14:07.743017361-08:00","closed_at":"2026-02-13T20:14:07.743017361-08:00","close_reason":"Validated strict true/false/omitted request-body regression coverage and passing strict serialization tests","labels":["openai","parity","strict","tests","tools"],"dependencies":[{"issue_id":"ai-sdk-rs-efl","depends_on_id":"ai-sdk-rs-9v0","type":"blocks","created_at":"2026-02-13T19:59:49.688416896-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-gsu","title":"Add transport init failure regression tests","description":"# atomic bead\n\n## full plan snapshot\nplan title: ai-sdk-rs fragility hardening and architecture LOC reduction\nplan summary: Harden known fragility points and reduce architecture duplication in provider stacks while preserving behavior parity and preventing coupling regression.\n\n### integration points (complete map)\n- [ip-openai-failure-semantics] OpenAI failure semantics\n  purpose: Ensure non-stream and stream failure paths return correct error/finish behavior.\n  integration contract: Non-stream top-level error must not return GenerateResponse success; stream failed trajectories must emit parity-aligned finish/error semantics.\n- [ip-transport-init-fallibility] Transport initialization fallibility\n  purpose: Prevent SDK process aborts from transport client init failures.\n  integration contract: Reqwest transport initialization surfaces recoverable errors instead of panicking.\n- [ip-google-stack-consolidation] Google and Vertex stack consolidation\n  purpose: Collapse duplicated provider implementations without behavior drift.\n  integration contract: Shared code preserves provider-specific auth, options scopes, and stream normalization semantics.\n- [ip-provider-bootstrap-dedupe] Provider bootstrap deduplication\n  purpose: Reduce repeated builder/header/default-option wiring across providers.\n  integration contract: Credential/env/header precedence and endpoint defaulting remain identical per provider.\n- [ip-error-mapping-unification] Error mapping unification\n  purpose: Unify fragmented error mapping with consistent diagnostics.\n  integration contract: Provider-specific parsing remains intact while fallback diagnostics become consistent and actionable.\n- [ip-module-boundary-normalization] Module boundary normalization\n  purpose: Reduce path-based boundary indirection and improve coupling observability.\n  integration contract: Public module exposure remains stable while internal structure improves analyzability and maintainability.\n\n### global constraints\n- No behavior or functionality loss.\n- Preserve public API contracts unless explicitly approved.\n- Keep provider request/response shapes and stream-part ordering stable.\n- Do not accept any coupling regression at the workspace gate.\n- Each slice must be validated with tests and quant_metrics snapshot/compare.\n\n### definition of done\n- OpenAI failure semantics match expected non-stream and stream parity behavior.\n- Transport initialization no longer panics on client build failure paths.\n- Google and Google Vertex duplicated stacks are consolidated behind shared modules with parity maintained.\n- Provider bootstrap and error-mapping duplication are reduced with equivalent behavior.\n- Path-based module boundary debt is reduced enough for coupling metrics to remain trustworthy for future slices.\n- quant_metrics compare verdict remains improves or no-regression for accepted slices.\n\n### plan assumptions\n- Current parity gap matrix for OpenAI responses reflects active known gaps.\n- inventory-based provider registration remains required.\n- Refactors can be staged incrementally without full rewrite.\n\n### plan risks\n- Provider parity regressions caused by shared-module extraction.\n- Hidden differences between Google GenAI and Google Vertex behavior during consolidation.\n- Coupling metric fidelity is partially limited by current path-based module structure.\n\n## bead-local scope\nbead key: reqwest-init-failure-tests\ntask: Add focused tests that assert fallible transport setup behavior and absence of panic in failure scenarios.\nintegration points touched: ip-transport-init-fallibility\nwhy: n/a\n\n### deliverables\n- Regression tests for transport init failure semantics\n- Test notes documenting expected recoverability behavior\n\n### acceptance\n- Tests validate non-panicking behavior\n- Error class and message assertions are stable\n\n### dependency intent\n- reqwest-transport-fallible-init\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:2452976 picked-at=2026-02-14T02:35:01Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-gsu.lock","status":"closed","priority":1,"issue_type":"task","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T18:24:56.222602901-08:00","created_by":"pomterre","updated_at":"2026-02-13T18:36:58.683462648-08:00","closed_at":"2026-02-13T18:36:58.683462648-08:00","close_reason":"Added reqwest transport init regression tests for deterministic client-build failure: try_new returns TransportError and fallback constructor remains non-panicking.","labels":["tests","transport"],"dependencies":[{"issue_id":"ai-sdk-rs-gsu","depends_on_id":"ai-sdk-rs-aaa","type":"blocks","created_at":"2026-02-13T18:25:44.580550823-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-h3z","title":"Add regression test for do_generate response.error handling","description":"# atomic bead\n\n## full plan snapshot\nplan title: OpenAI Responses parity remediation for ai-sdk-rs\nplan summary: Close behavior drift between ai-sdk-rs OpenAI Responses provider and /home/mike/ai by fixing non-stream error propagation, response.failed streaming finish semantics, usage detail mapping, and function tool strict passthrough, with fixture-based regression coverage.\n\n### integration points (complete map)\n- [ip-nonstream-response-error] Non-stream response.error propagation\n  purpose: Ensure do_generate fails when OpenAI response payload contains top-level response.error, matching TS provider behavior.\n  integration contract: Given a response with response.error, ai-sdk-rs must surface an error instead of returning normal content.\n- [ip-stream-failed-finish] Streaming response.failed finish semantics\n  purpose: Align response.failed/error stream terminal behavior with TS finish reason semantics.\n  integration contract: For error/failed stream trajectories, finish reason and terminal parts match TS parity expectations.\n- [ip-usage-details] Usage details normalization\n  purpose: Map nested usage detail fields (cached and reasoning token details) into normalized usage outputs.\n  integration contract: input_tokens_details.cached_tokens and output_tokens_details.reasoning_tokens are preserved in normalized usage fields.\n- [ip-function-tool-strict] Function tool strict passthrough\n  purpose: Preserve function tool strict mode when building Responses API request bodies.\n  integration contract: Function tools include strict=true/false when specified and omit strict when unspecified.\n\n### global constraints\n- Scope is existing provider only: openai responses path in ai-sdk-rs.\n- Use fixture-based tests; no live provider API calls.\n- Preserve normalized stream contract in crates/sdk-types/src/v2.rs and avoid introducing incompatible stream part shapes.\n- Keep fixes localized to OpenAI provider/core mapping paths unless type support requires shared sdk-types changes.\n\n### definition of done\n- All four parity gaps are covered by deterministic tests and passing implementations.\n- OpenAI provider targeted tests pass for both request serialization and stream normalization paths.\n- No dependency cycles exist in bead graph and execution order reflects blocker intent.\n- Final parity note lists resolved gaps and any residual risk with file-level references.\n\n### plan assumptions\n- Current TS behavior in /home/mike/ai/packages/openai/src/responses is the parity baseline.\n- OpenAI fixture corpus in crates/providers/openai/tests/fixtures is authoritative for offline validation.\n- Rust stream finish behavior should align with TS snapshots for response.failed/error trajectories.\n\n### plan risks\n- Adjusting finish semantics may affect downstream consumers that implicitly depended on previous finish reasons.\n- Adding strict support may require shared type updates that can affect other providers if not isolated.\n- Usage mapping changes could unintentionally alter reported token accounting in unrelated tests.\n\n## bead-local scope\nbead key: test-do-generate-response-error\ntask: Add a failing regression test asserting OpenAI do_generate returns an error when response.error is present, using existing offline error fixtures.\nintegration points touched: ip-nonstream-response-error\nwhy: Locks expected behavior before implementation changes and prevents silent regressions.\n\n### deliverables\n- New/updated test in crates/providers/openai/tests/responses_language_model_tests.rs\n- Assertion that generation path throws/returns SdkError on response.error\n\n### acceptance\n- Test fails on current behavior and encodes TS parity expectation\n- Test is fixture-based and deterministic\n\n### dependency intent\n- openai-parity-gap-matrix\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:2338613 picked-at=2026-02-14T02:10:26Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-h3z.lock","status":"closed","priority":1,"issue_type":"task","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T18:02:05.050432674-08:00","created_by":"pomterre","updated_at":"2026-02-13T18:13:15.107245345-08:00","closed_at":"2026-02-13T18:13:15.107245345-08:00","close_reason":"Added fixture-based failing regression test for do_generate response.error propagation","labels":["openai","parity","tests"],"dependencies":[{"issue_id":"ai-sdk-rs-h3z","depends_on_id":"ai-sdk-rs-8uf","type":"blocks","created_at":"2026-02-13T18:02:43.758212661-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-ika","title":"Implement nested usage details mapping","description":"# atomic bead\n\n## full plan snapshot\nplan title: OpenAI Responses parity gap closure (atomized)\nplan summary: Close the OpenAI Responses parity gaps against /home/mike/ai/packages/openai/src/responses by landing error-path, finish semantics, usage mapping, and function tool strict-mode parity with fixture-backed regressions.\n\n### integration points (complete map)\n- [ip-nonstream-response-error] Non-stream top-level error behavior\n  purpose: Ensure do_generate fails when response payload contains top-level error, matching TS behavior.\n  integration contract: If response.error exists, return SdkError::Upstream instead of GenerateResponse.\n- [ip-stream-failed-finish] Stream failed-finish semantics\n  purpose: Align response.failed stream trajectories with TS finish semantics and metadata invariants.\n  integration contract: response.failed must not be treated as response.completed/incomplete; finish reason remains Other with correct metadata behavior.\n- [ip-usage-details] Nested usage details mapping\n  purpose: Map cached and reasoning token details from nested provider usage structures.\n  integration contract: input_tokens_details.cached_tokens -\u003e Usage.cached_input_tokens and output_tokens_details.reasoning_tokens -\u003e Usage.reasoning_tokens.\n- [ip-function-tool-strict] Function tool strict passthrough\n  purpose: Preserve strict true/false/unset semantics in function tool request serialization.\n  integration contract: Serialize strict when explicitly true/false and omit when unset, matching TS prepare-tools behavior.\n\n### global constraints\n- Existing provider only: openai responses path in ai-sdk-rs; do not introduce new providers.\n- Prefer fixture-based tests; do not call live provider APIs.\n- Preserve normalized StreamPart/Event semantics and keep TS baseline behavior authoritative.\n- Keep changes scoped to openai provider and sdk-types only where required for strict parity.\n\n### definition of done\n- All beads reference at least one integration point and can execute independently with local context.\n- OpenAI parity regressions for the targeted behaviors are covered by deterministic tests.\n- cargo test --lib provider_openai:: passes for updated tests.\n- Dependency graph has no cycles and intended blocker relationships are encoded.\n\n### plan assumptions\n- docs/plans/openai-responses-parity-gap-matrix.md is the source plan for this atomization.\n- Fixture files under crates/providers/openai/tests/fixtures remain available and stable.\n- TS baseline files under /home/mike/ai/packages/openai/src/responses remain the parity reference.\n\n### plan risks\n- Finish-reason handling changes can regress stream ordering if hooks are altered incorrectly.\n- Usage mapping can silently drift if nested token fields are not tested in both stream and non-stream paths.\n- Function strict-mode parity can regress if provider options fallback and typed field behavior diverge.\n\n## bead-local scope\nbead key: usage-details-mapping-impl\ntask: Map nested usage token details into Usage.cached_input_tokens and Usage.reasoning_tokens in non-stream and stream paths.\nintegration points touched: ip-usage-details\nwhy: TS parity requires preserving nested cached/reasoning token information.\n\n### deliverables\n- Usage parsing/apply logic for input_tokens_details.cached_tokens\n- Usage parsing/apply logic for output_tokens_details.reasoning_tokens\n\n### acceptance\n- Both fields are populated when nested usage fields exist\n- No regression to existing usage totals\n\n### dependency intent\n- stream-failed-finish-fixture-tests\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:2909196 picked-at=2026-02-14T04:10:56Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-ika.lock","status":"closed","priority":1,"issue_type":"task","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T19:59:48.881033331-08:00","created_by":"pomterre","updated_at":"2026-02-13T20:11:32.172194771-08:00","closed_at":"2026-02-13T20:11:32.172194771-08:00","close_reason":"Verified nested usage details mapping remains implemented in non-stream and stream paths; cached_input_tokens and reasoning_tokens regressions pass","labels":["impl","openai","parity","usage"],"dependencies":[{"issue_id":"ai-sdk-rs-ika","depends_on_id":"ai-sdk-rs-1gj","type":"blocks","created_at":"2026-02-13T19:59:49.464281812-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-jmd","title":"Implement function tool strict passthrough in OpenAI requests","description":"# atomic bead\n\n## full plan snapshot\nplan title: OpenAI Responses parity remediation for ai-sdk-rs\nplan summary: Close behavior drift between ai-sdk-rs OpenAI Responses provider and /home/mike/ai by fixing non-stream error propagation, response.failed streaming finish semantics, usage detail mapping, and function tool strict passthrough, with fixture-based regression coverage.\n\n### integration points (complete map)\n- [ip-nonstream-response-error] Non-stream response.error propagation\n  purpose: Ensure do_generate fails when OpenAI response payload contains top-level response.error, matching TS provider behavior.\n  integration contract: Given a response with response.error, ai-sdk-rs must surface an error instead of returning normal content.\n- [ip-stream-failed-finish] Streaming response.failed finish semantics\n  purpose: Align response.failed/error stream terminal behavior with TS finish reason semantics.\n  integration contract: For error/failed stream trajectories, finish reason and terminal parts match TS parity expectations.\n- [ip-usage-details] Usage details normalization\n  purpose: Map nested usage detail fields (cached and reasoning token details) into normalized usage outputs.\n  integration contract: input_tokens_details.cached_tokens and output_tokens_details.reasoning_tokens are preserved in normalized usage fields.\n- [ip-function-tool-strict] Function tool strict passthrough\n  purpose: Preserve function tool strict mode when building Responses API request bodies.\n  integration contract: Function tools include strict=true/false when specified and omit strict when unspecified.\n\n### global constraints\n- Scope is existing provider only: openai responses path in ai-sdk-rs.\n- Use fixture-based tests; no live provider API calls.\n- Preserve normalized stream contract in crates/sdk-types/src/v2.rs and avoid introducing incompatible stream part shapes.\n- Keep fixes localized to OpenAI provider/core mapping paths unless type support requires shared sdk-types changes.\n\n### definition of done\n- All four parity gaps are covered by deterministic tests and passing implementations.\n- OpenAI provider targeted tests pass for both request serialization and stream normalization paths.\n- No dependency cycles exist in bead graph and execution order reflects blocker intent.\n- Final parity note lists resolved gaps and any residual risk with file-level references.\n\n### plan assumptions\n- Current TS behavior in /home/mike/ai/packages/openai/src/responses is the parity baseline.\n- OpenAI fixture corpus in crates/providers/openai/tests/fixtures is authoritative for offline validation.\n- Rust stream finish behavior should align with TS snapshots for response.failed/error trajectories.\n\n### plan risks\n- Adjusting finish semantics may affect downstream consumers that implicitly depended on previous finish reasons.\n- Adding strict support may require shared type updates that can affect other providers if not isolated.\n- Usage mapping changes could unintentionally alter reported token accounting in unrelated tests.\n\n## bead-local scope\nbead key: fix-function-tool-strict-passthrough\ntask: Extend relevant Rust types and OpenAI request builder serialization to preserve function tool strict mode exactly when provided.\nintegration points touched: ip-function-tool-strict\nwhy: n/a\n\n### deliverables\n- Type/model update for strict support where needed\n- OpenAI request serialization update to include strict when present\n\n### acceptance\n- Strict passthrough request-body tests pass\n- No breakage in existing function tool serialization tests\n\n### dependency intent\n- test-function-tool-strict-passthrough\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:2467496 picked-at=2026-02-14T02:39:09Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-jmd.lock","status":"closed","priority":0,"issue_type":"feature","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T18:02:05.388440132-08:00","created_by":"pomterre","updated_at":"2026-02-13T18:43:17.475100947-08:00","closed_at":"2026-02-13T18:43:17.475100947-08:00","close_reason":"Implemented FunctionTool strict passthrough in OpenAI Responses request bodies with regression coverage; added strict field propagation and compatibility initializers.","labels":["openai","parity","serialization"],"dependencies":[{"issue_id":"ai-sdk-rs-jmd","depends_on_id":"ai-sdk-rs-vvn","type":"blocks","created_at":"2026-02-13T18:02:44.057154265-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-ke2","title":"Unify provider error mapper fallback behavior","description":"# atomic bead\n\n## full plan snapshot\nplan title: ai-sdk-rs fragility hardening and architecture LOC reduction\nplan summary: Harden known fragility points and reduce architecture duplication in provider stacks while preserving behavior parity and preventing coupling regression.\n\n### integration points (complete map)\n- [ip-openai-failure-semantics] OpenAI failure semantics\n  purpose: Ensure non-stream and stream failure paths return correct error/finish behavior.\n  integration contract: Non-stream top-level error must not return GenerateResponse success; stream failed trajectories must emit parity-aligned finish/error semantics.\n- [ip-transport-init-fallibility] Transport initialization fallibility\n  purpose: Prevent SDK process aborts from transport client init failures.\n  integration contract: Reqwest transport initialization surfaces recoverable errors instead of panicking.\n- [ip-google-stack-consolidation] Google and Vertex stack consolidation\n  purpose: Collapse duplicated provider implementations without behavior drift.\n  integration contract: Shared code preserves provider-specific auth, options scopes, and stream normalization semantics.\n- [ip-provider-bootstrap-dedupe] Provider bootstrap deduplication\n  purpose: Reduce repeated builder/header/default-option wiring across providers.\n  integration contract: Credential/env/header precedence and endpoint defaulting remain identical per provider.\n- [ip-error-mapping-unification] Error mapping unification\n  purpose: Unify fragmented error mapping with consistent diagnostics.\n  integration contract: Provider-specific parsing remains intact while fallback diagnostics become consistent and actionable.\n- [ip-module-boundary-normalization] Module boundary normalization\n  purpose: Reduce path-based boundary indirection and improve coupling observability.\n  integration contract: Public module exposure remains stable while internal structure improves analyzability and maintainability.\n\n### global constraints\n- No behavior or functionality loss.\n- Preserve public API contracts unless explicitly approved.\n- Keep provider request/response shapes and stream-part ordering stable.\n- Do not accept any coupling regression at the workspace gate.\n- Each slice must be validated with tests and quant_metrics snapshot/compare.\n\n### definition of done\n- OpenAI failure semantics match expected non-stream and stream parity behavior.\n- Transport initialization no longer panics on client build failure paths.\n- Google and Google Vertex duplicated stacks are consolidated behind shared modules with parity maintained.\n- Provider bootstrap and error-mapping duplication are reduced with equivalent behavior.\n- Path-based module boundary debt is reduced enough for coupling metrics to remain trustworthy for future slices.\n- quant_metrics compare verdict remains improves or no-regression for accepted slices.\n\n### plan assumptions\n- Current parity gap matrix for OpenAI responses reflects active known gaps.\n- inventory-based provider registration remains required.\n- Refactors can be staged incrementally without full rewrite.\n\n### plan risks\n- Provider parity regressions caused by shared-module extraction.\n- Hidden differences between Google GenAI and Google Vertex behavior during consolidation.\n- Coupling metric fidelity is partially limited by current path-based module structure.\n\n## bead-local scope\nbead key: provider-error-mapper-unification\ntask: Centralize common transport HttpStatus fallback mapping and preserve provider-specific body parsing hooks.\nintegration points touched: ip-error-mapping-unification\nwhy: Current fragmented mappers produce inconsistent diagnostics and duplicate code.\n\n### deliverables\n- Shared fallback mapper path\n- Provider-specific parser hooks retained for message extraction\n\n### acceptance\n- Fallback diagnostics are consistent across providers\n- Existing provider-specific parsed errors are preserved\n\n### dependency intent\n- provider-bootstrap-shared-scaffold\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:2608557 picked-at=2026-02-14T03:22:53Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-ke2.lock","status":"closed","priority":2,"issue_type":"feature","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T18:24:56.630375479-08:00","created_by":"pomterre","updated_at":"2026-02-13T19:27:57.504775237-08:00","closed_at":"2026-02-13T19:27:57.504775237-08:00","close_reason":"Centralized shared HttpStatus fallback mapping while preserving provider-specific parsed error message extraction","labels":["architecture","errors","ux"],"dependencies":[{"issue_id":"ai-sdk-rs-ke2","depends_on_id":"ai-sdk-rs-9vu","type":"blocks","created_at":"2026-02-13T18:25:44.941027972-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-mpj","title":"Align OpenAI failed-stream finish semantics","description":"# atomic bead\n\n## full plan snapshot\nplan title: ai-sdk-rs fragility hardening and architecture LOC reduction\nplan summary: Harden known fragility points and reduce architecture duplication in provider stacks while preserving behavior parity and preventing coupling regression.\n\n### integration points (complete map)\n- [ip-openai-failure-semantics] OpenAI failure semantics\n  purpose: Ensure non-stream and stream failure paths return correct error/finish behavior.\n  integration contract: Non-stream top-level error must not return GenerateResponse success; stream failed trajectories must emit parity-aligned finish/error semantics.\n- [ip-transport-init-fallibility] Transport initialization fallibility\n  purpose: Prevent SDK process aborts from transport client init failures.\n  integration contract: Reqwest transport initialization surfaces recoverable errors instead of panicking.\n- [ip-google-stack-consolidation] Google and Vertex stack consolidation\n  purpose: Collapse duplicated provider implementations without behavior drift.\n  integration contract: Shared code preserves provider-specific auth, options scopes, and stream normalization semantics.\n- [ip-provider-bootstrap-dedupe] Provider bootstrap deduplication\n  purpose: Reduce repeated builder/header/default-option wiring across providers.\n  integration contract: Credential/env/header precedence and endpoint defaulting remain identical per provider.\n- [ip-error-mapping-unification] Error mapping unification\n  purpose: Unify fragmented error mapping with consistent diagnostics.\n  integration contract: Provider-specific parsing remains intact while fallback diagnostics become consistent and actionable.\n- [ip-module-boundary-normalization] Module boundary normalization\n  purpose: Reduce path-based boundary indirection and improve coupling observability.\n  integration contract: Public module exposure remains stable while internal structure improves analyzability and maintainability.\n\n### global constraints\n- No behavior or functionality loss.\n- Preserve public API contracts unless explicitly approved.\n- Keep provider request/response shapes and stream-part ordering stable.\n- Do not accept any coupling regression at the workspace gate.\n- Each slice must be validated with tests and quant_metrics snapshot/compare.\n\n### definition of done\n- OpenAI failure semantics match expected non-stream and stream parity behavior.\n- Transport initialization no longer panics on client build failure paths.\n- Google and Google Vertex duplicated stacks are consolidated behind shared modules with parity maintained.\n- Provider bootstrap and error-mapping duplication are reduced with equivalent behavior.\n- Path-based module boundary debt is reduced enough for coupling metrics to remain trustworthy for future slices.\n- quant_metrics compare verdict remains improves or no-regression for accepted slices.\n\n### plan assumptions\n- Current parity gap matrix for OpenAI responses reflects active known gaps.\n- inventory-based provider registration remains required.\n- Refactors can be staged incrementally without full rewrite.\n\n### plan risks\n- Provider parity regressions caused by shared-module extraction.\n- Hidden differences between Google GenAI and Google Vertex behavior during consolidation.\n- Coupling metric fidelity is partially limited by current path-based module structure.\n\n## bead-local scope\nbead key: openai-stream-failed-finish-fix\ntask: Adjust stream mapping so response.failed and related error events emit parity-aligned finish semantics and metadata.\nintegration points touched: ip-openai-failure-semantics\nwhy: Failed streams currently risk being treated as normal completion.\n\n### deliverables\n- Stream event/finish mapping changes for failed trajectories\n- Fixture assertions for finish reason and error emissions\n\n### acceptance\n- openai-error stream fixture validates expected finish semantics\n- No regressions in normal completed/incomplete stream paths\n\n### dependency intent\n- openai-failure-char-tests\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:2459411 picked-at=2026-02-14T02:37:29Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-mpj.lock","status":"closed","priority":0,"issue_type":"bug","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T18:24:56.000120786-08:00","created_by":"pomterre","updated_at":"2026-02-13T18:38:49.931225308-08:00","closed_at":"2026-02-13T18:38:49.931225308-08:00","close_reason":"Validated failed-stream parity behavior with fixture regressions: stream_error_fixture now passes and normal stream fixture remains green.","labels":["fragility","openai","streaming"],"dependencies":[{"issue_id":"ai-sdk-rs-mpj","depends_on_id":"ai-sdk-rs-qw4","type":"blocks","created_at":"2026-02-13T18:25:44.48842796-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-oix","title":"Add explicit Groq provider registration in ai-sdk-rs","description":"# atomic bead\n\n## full plan snapshot\nplan title: Provider Expansion Plan: Catalog-Normalized + ai-sdk-rs Registry Coverage\nplan summary: Normalize provider sdk labels at the Nodecode gateway appendix source and expand ai-sdk-rs provider routing so catalog providers resolve through existing implementations, starting with explicit Groq coverage and common OpenAI-compatible provider IDs.\n\n### integration points (complete map)\n- [ip-upstream-catalog-normalization] Gateway appendix sdk normalization\n  purpose: Guarantee emitted provider sdk values are canonical for ai-sdk-rs routing.\n  integration contract: appendix providers[].sdk values must be from the ai-sdk-rs-supported sdk set and preserve groq as groq.\n- [ip-openai-compatible-provider-id-routing] OpenAI-compatible provider-id routing in ai-sdk-rs\n  purpose: Resolve common provider IDs directly to OpenAI-compatible builder behavior.\n  integration contract: sdk_type_from_id and registry coverage must include target provider IDs with deterministic routing.\n- [ip-groq-routing] Explicit Groq routing support\n  purpose: Route sdk_type=groq catalog providers through a supported builder path.\n  integration contract: Groq provider definition must build a language model without fallback ambiguity.\n- [ip-nodecode-catalog-path] Nodecode catalog path precedence\n  purpose: Align runtime catalog reads to ~/.cache/nodecode/providers/catalog.json with safe fallback.\n  integration contract: path/env precedence is deterministic and test-covered for both nodecode and legacy paths.\n- [ip-fixture-parity-coverage] Fixture-based parity coverage for expanded provider IDs\n  purpose: Prevent request/stream/error mapping regressions while widening provider ID coverage.\n  integration contract: offline fixtures assert request serialization, stream normalization, and error mapping invariants.\n\n### global constraints\n- Preserve existing provider behavior for openai/anthropic/google/google-vertex/amazon-bedrock.\n- Use fixture-based parity tests only; no live provider API calls.\n- Keep Rust changes limited to currently representable interfaces: language, embeddings, images.\n- Any provider sdk label emitted by appendix must be parseable and routable by ai-sdk-rs without ad-hoc fallbacks.\n\n### definition of done\n- Gateway appendix export emits normalized sdk labels compatible with ai-sdk-rs.\n- ai-sdk-rs resolves and builds models for Groq and selected OpenAI-compatible provider IDs.\n- Capabilities loader prefers ~/.cache/nodecode/providers/catalog.json with env override precedence.\n- New regression tests cover sdk normalization, registry resolution, path precedence, and provider parity fixtures.\n- Dependency graph has no cycles and all created beads are ready/traceable.\n\n### plan assumptions\n- Catalog source of truth is generated in ~/nodecode-gateway-1 via modrepo appendix tooling.\n- Provider-specific TS surfaces without Rust interfaces (for example transcription/reranking) are explicitly deferred.\n- openai-compatible Rust implementation remains the shared execution path for aliased provider IDs.\n\n### plan risks\n- Unnormalized sdk labels can silently break provider dispatch at runtime.\n- Alias expansion may regress provider option scoping if provider_scope_name handling drifts.\n- Cache path changes can cause environment-specific behavior if precedence is not tested.\n\n## bead-local scope\nbead key: ai-sdk-rs-groq-registration\ntask: Register Groq in crates/providers/openai-compatible/src/provider.rs with sdk_type SdkType::Groq routed to existing OpenAI-compatible chat builder.\nintegration points touched: ip-groq-routing\nwhy: Groq is the active catalog sdk gap that currently lacks explicit builder coverage.\n\n### deliverables\n- new ProviderRegistration entry for groq\n- routing behavior documented in provider registry tests\n\n### acceptance\n- provider definition sdk_type=groq builds a language model successfully\n- no regressions in existing openai-compatible registrations\n\n### dependency intent\n- provider-expansion-baseline-matrix\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=codex picked-at=2026-02-14T04:51:37Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-oix.lock","status":"closed","priority":0,"issue_type":"feature","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T20:33:08.946739411-08:00","created_by":"pomterre","updated_at":"2026-02-13T20:54:11.282271068-08:00","closed_at":"2026-02-13T20:54:11.282271068-08:00","close_reason":"Added explicit groq ProviderRegistration mapped to OpenAI-compatible chat builder with registry regression coverage.","labels":["ai-sdk-rs","groq"],"dependencies":[{"issue_id":"ai-sdk-rs-oix","depends_on_id":"ai-sdk-rs-am3","type":"blocks","created_at":"2026-02-13T20:33:09.407584098-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-pfo","title":"Implement do_generate response.error propagation parity","description":"# atomic bead\n\n## full plan snapshot\nplan title: OpenAI Responses parity remediation for ai-sdk-rs\nplan summary: Close behavior drift between ai-sdk-rs OpenAI Responses provider and /home/mike/ai by fixing non-stream error propagation, response.failed streaming finish semantics, usage detail mapping, and function tool strict passthrough, with fixture-based regression coverage.\n\n### integration points (complete map)\n- [ip-nonstream-response-error] Non-stream response.error propagation\n  purpose: Ensure do_generate fails when OpenAI response payload contains top-level response.error, matching TS provider behavior.\n  integration contract: Given a response with response.error, ai-sdk-rs must surface an error instead of returning normal content.\n- [ip-stream-failed-finish] Streaming response.failed finish semantics\n  purpose: Align response.failed/error stream terminal behavior with TS finish reason semantics.\n  integration contract: For error/failed stream trajectories, finish reason and terminal parts match TS parity expectations.\n- [ip-usage-details] Usage details normalization\n  purpose: Map nested usage detail fields (cached and reasoning token details) into normalized usage outputs.\n  integration contract: input_tokens_details.cached_tokens and output_tokens_details.reasoning_tokens are preserved in normalized usage fields.\n- [ip-function-tool-strict] Function tool strict passthrough\n  purpose: Preserve function tool strict mode when building Responses API request bodies.\n  integration contract: Function tools include strict=true/false when specified and omit strict when unspecified.\n\n### global constraints\n- Scope is existing provider only: openai responses path in ai-sdk-rs.\n- Use fixture-based tests; no live provider API calls.\n- Preserve normalized stream contract in crates/sdk-types/src/v2.rs and avoid introducing incompatible stream part shapes.\n- Keep fixes localized to OpenAI provider/core mapping paths unless type support requires shared sdk-types changes.\n\n### definition of done\n- All four parity gaps are covered by deterministic tests and passing implementations.\n- OpenAI provider targeted tests pass for both request serialization and stream normalization paths.\n- No dependency cycles exist in bead graph and execution order reflects blocker intent.\n- Final parity note lists resolved gaps and any residual risk with file-level references.\n\n### plan assumptions\n- Current TS behavior in /home/mike/ai/packages/openai/src/responses is the parity baseline.\n- OpenAI fixture corpus in crates/providers/openai/tests/fixtures is authoritative for offline validation.\n- Rust stream finish behavior should align with TS snapshots for response.failed/error trajectories.\n\n### plan risks\n- Adjusting finish semantics may affect downstream consumers that implicitly depended on previous finish reasons.\n- Adding strict support may require shared type updates that can affect other providers if not isolated.\n- Usage mapping changes could unintentionally alter reported token accounting in unrelated tests.\n\n## bead-local scope\nbead key: fix-do-generate-response-error\ntask: Update OpenAI Responses non-stream path to detect response.error and return mapped error instead of normal content.\nintegration points touched: ip-nonstream-response-error\nwhy: n/a\n\n### deliverables\n- Code change in crates/providers/openai/src/responses/language_model.rs\n- Any required error mapping helper adjustments\n\n### acceptance\n- Regression test for response.error handling passes\n- No unrelated non-stream OpenAI tests regress\n\n### dependency intent\n- test-do-generate-response-error\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:2380759 picked-at=2026-02-14T02:23:01Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-pfo.lock","status":"closed","priority":0,"issue_type":"bug","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T18:02:05.094327796-08:00","created_by":"pomterre","updated_at":"2026-02-13T18:24:57.261976317-08:00","closed_at":"2026-02-13T18:24:57.261976317-08:00","close_reason":"do_generate now surfaces top-level response.error as SdkError::Upstream(status=400) before content extraction, matching TS non-stream parity.","labels":["bugfix","openai","parity"],"dependencies":[{"issue_id":"ai-sdk-rs-pfo","depends_on_id":"ai-sdk-rs-h3z","type":"blocks","created_at":"2026-02-13T18:02:43.800908596-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-qw4","title":"Characterize OpenAI failure semantics with regression fixtures","description":"# atomic bead\n\n## full plan snapshot\nplan title: ai-sdk-rs fragility hardening and architecture LOC reduction\nplan summary: Harden known fragility points and reduce architecture duplication in provider stacks while preserving behavior parity and preventing coupling regression.\n\n### integration points (complete map)\n- [ip-openai-failure-semantics] OpenAI failure semantics\n  purpose: Ensure non-stream and stream failure paths return correct error/finish behavior.\n  integration contract: Non-stream top-level error must not return GenerateResponse success; stream failed trajectories must emit parity-aligned finish/error semantics.\n- [ip-transport-init-fallibility] Transport initialization fallibility\n  purpose: Prevent SDK process aborts from transport client init failures.\n  integration contract: Reqwest transport initialization surfaces recoverable errors instead of panicking.\n- [ip-google-stack-consolidation] Google and Vertex stack consolidation\n  purpose: Collapse duplicated provider implementations without behavior drift.\n  integration contract: Shared code preserves provider-specific auth, options scopes, and stream normalization semantics.\n- [ip-provider-bootstrap-dedupe] Provider bootstrap deduplication\n  purpose: Reduce repeated builder/header/default-option wiring across providers.\n  integration contract: Credential/env/header precedence and endpoint defaulting remain identical per provider.\n- [ip-error-mapping-unification] Error mapping unification\n  purpose: Unify fragmented error mapping with consistent diagnostics.\n  integration contract: Provider-specific parsing remains intact while fallback diagnostics become consistent and actionable.\n- [ip-module-boundary-normalization] Module boundary normalization\n  purpose: Reduce path-based boundary indirection and improve coupling observability.\n  integration contract: Public module exposure remains stable while internal structure improves analyzability and maintainability.\n\n### global constraints\n- No behavior or functionality loss.\n- Preserve public API contracts unless explicitly approved.\n- Keep provider request/response shapes and stream-part ordering stable.\n- Do not accept any coupling regression at the workspace gate.\n- Each slice must be validated with tests and quant_metrics snapshot/compare.\n\n### definition of done\n- OpenAI failure semantics match expected non-stream and stream parity behavior.\n- Transport initialization no longer panics on client build failure paths.\n- Google and Google Vertex duplicated stacks are consolidated behind shared modules with parity maintained.\n- Provider bootstrap and error-mapping duplication are reduced with equivalent behavior.\n- Path-based module boundary debt is reduced enough for coupling metrics to remain trustworthy for future slices.\n- quant_metrics compare verdict remains improves or no-regression for accepted slices.\n\n### plan assumptions\n- Current parity gap matrix for OpenAI responses reflects active known gaps.\n- inventory-based provider registration remains required.\n- Refactors can be staged incrementally without full rewrite.\n\n### plan risks\n- Provider parity regressions caused by shared-module extraction.\n- Hidden differences between Google GenAI and Google Vertex behavior during consolidation.\n- Coupling metric fidelity is partially limited by current path-based module structure.\n\n## bead-local scope\nbead key: openai-failure-char-tests\ntask: Codify current non-stream and stream failure trajectories as characterization tests and expected parity assertions before behavior edits.\nintegration points touched: ip-openai-failure-semantics\nwhy: Prevents accidental behavior drift while fixing known fragility.\n\n### deliverables\n- Fixture-backed tests covering top-level response.error and response.failed stream paths\n- Documented expected finish/error mapping semantics\n\n### acceptance\n- Tests fail on current known-bad behavior and pass when corrected\n- Assertions cover both do_generate and do_stream trajectories\n\n### dependency intent\n- none\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:2386956 picked-at=2026-02-14T02:25:22Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-qw4.lock","status":"closed","priority":0,"issue_type":"task","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T18:24:55.818795129-08:00","created_by":"pomterre","updated_at":"2026-02-13T18:25:54.625419439-08:00","closed_at":"2026-02-13T18:25:54.625419439-08:00","close_reason":"Characterization coverage already present: non_stream_response_error_returns_error passes for fixed do_generate path, while stream_error_fixture still fails on known response.failed parity drift (serviceTier metadata).","labels":["fragility","openai","parity"]}
{"id":"ai-sdk-rs-r9n","title":"Add fixture parity tests for expanded provider-id coverage","description":"# atomic bead\n\n## full plan snapshot\nplan title: Provider Expansion Plan: Catalog-Normalized + ai-sdk-rs Registry Coverage\nplan summary: Normalize provider sdk labels at the Nodecode gateway appendix source and expand ai-sdk-rs provider routing so catalog providers resolve through existing implementations, starting with explicit Groq coverage and common OpenAI-compatible provider IDs.\n\n### integration points (complete map)\n- [ip-upstream-catalog-normalization] Gateway appendix sdk normalization\n  purpose: Guarantee emitted provider sdk values are canonical for ai-sdk-rs routing.\n  integration contract: appendix providers[].sdk values must be from the ai-sdk-rs-supported sdk set and preserve groq as groq.\n- [ip-openai-compatible-provider-id-routing] OpenAI-compatible provider-id routing in ai-sdk-rs\n  purpose: Resolve common provider IDs directly to OpenAI-compatible builder behavior.\n  integration contract: sdk_type_from_id and registry coverage must include target provider IDs with deterministic routing.\n- [ip-groq-routing] Explicit Groq routing support\n  purpose: Route sdk_type=groq catalog providers through a supported builder path.\n  integration contract: Groq provider definition must build a language model without fallback ambiguity.\n- [ip-nodecode-catalog-path] Nodecode catalog path precedence\n  purpose: Align runtime catalog reads to ~/.cache/nodecode/providers/catalog.json with safe fallback.\n  integration contract: path/env precedence is deterministic and test-covered for both nodecode and legacy paths.\n- [ip-fixture-parity-coverage] Fixture-based parity coverage for expanded provider IDs\n  purpose: Prevent request/stream/error mapping regressions while widening provider ID coverage.\n  integration contract: offline fixtures assert request serialization, stream normalization, and error mapping invariants.\n\n### global constraints\n- Preserve existing provider behavior for openai/anthropic/google/google-vertex/amazon-bedrock.\n- Use fixture-based parity tests only; no live provider API calls.\n- Keep Rust changes limited to currently representable interfaces: language, embeddings, images.\n- Any provider sdk label emitted by appendix must be parseable and routable by ai-sdk-rs without ad-hoc fallbacks.\n\n### definition of done\n- Gateway appendix export emits normalized sdk labels compatible with ai-sdk-rs.\n- ai-sdk-rs resolves and builds models for Groq and selected OpenAI-compatible provider IDs.\n- Capabilities loader prefers ~/.cache/nodecode/providers/catalog.json with env override precedence.\n- New regression tests cover sdk normalization, registry resolution, path precedence, and provider parity fixtures.\n- Dependency graph has no cycles and all created beads are ready/traceable.\n\n### plan assumptions\n- Catalog source of truth is generated in ~/nodecode-gateway-1 via modrepo appendix tooling.\n- Provider-specific TS surfaces without Rust interfaces (for example transcription/reranking) are explicitly deferred.\n- openai-compatible Rust implementation remains the shared execution path for aliased provider IDs.\n\n### plan risks\n- Unnormalized sdk labels can silently break provider dispatch at runtime.\n- Alias expansion may regress provider option scoping if provider_scope_name handling drifts.\n- Cache path changes can cause environment-specific behavior if precedence is not tested.\n\n## bead-local scope\nbead key: ai-sdk-rs-openai-compatible-parity-fixtures\ntask: Add fixture-driven tests in openai-compatible test suite for request serialization, stream normalization, and error mapping across newly routed provider IDs.\nintegration points touched: ip-fixture-parity-coverage, ip-openai-compatible-provider-id-routing\nwhy: Widened provider-id coverage must not regress core parity behavior.\n\n### deliverables\n- offline fixtures and assertions for selected provider IDs\n- coverage of request/stream/error parity invariants\n\n### acceptance\n- tests run without network access\n- stream and error semantics match existing openai-compatible invariants\n\n### dependency intent\n- ai-sdk-rs-openai-compatible-aliases\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","status":"open","priority":2,"issue_type":"task","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T20:33:09.216390019-08:00","created_by":"pomterre","updated_at":"2026-02-13T20:33:09.216390019-08:00","labels":["ai-sdk-rs","fixtures","parity"],"dependencies":[{"issue_id":"ai-sdk-rs-r9n","depends_on_id":"ai-sdk-rs-101","type":"blocks","created_at":"2026-02-13T20:33:09.708069288-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-r9t","title":"Consolidate Google and Vertex language-model stream core","description":"# atomic bead\n\n## full plan snapshot\nplan title: ai-sdk-rs fragility hardening and architecture LOC reduction\nplan summary: Harden known fragility points and reduce architecture duplication in provider stacks while preserving behavior parity and preventing coupling regression.\n\n### integration points (complete map)\n- [ip-openai-failure-semantics] OpenAI failure semantics\n  purpose: Ensure non-stream and stream failure paths return correct error/finish behavior.\n  integration contract: Non-stream top-level error must not return GenerateResponse success; stream failed trajectories must emit parity-aligned finish/error semantics.\n- [ip-transport-init-fallibility] Transport initialization fallibility\n  purpose: Prevent SDK process aborts from transport client init failures.\n  integration contract: Reqwest transport initialization surfaces recoverable errors instead of panicking.\n- [ip-google-stack-consolidation] Google and Vertex stack consolidation\n  purpose: Collapse duplicated provider implementations without behavior drift.\n  integration contract: Shared code preserves provider-specific auth, options scopes, and stream normalization semantics.\n- [ip-provider-bootstrap-dedupe] Provider bootstrap deduplication\n  purpose: Reduce repeated builder/header/default-option wiring across providers.\n  integration contract: Credential/env/header precedence and endpoint defaulting remain identical per provider.\n- [ip-error-mapping-unification] Error mapping unification\n  purpose: Unify fragmented error mapping with consistent diagnostics.\n  integration contract: Provider-specific parsing remains intact while fallback diagnostics become consistent and actionable.\n- [ip-module-boundary-normalization] Module boundary normalization\n  purpose: Reduce path-based boundary indirection and improve coupling observability.\n  integration contract: Public module exposure remains stable while internal structure improves analyzability and maintainability.\n\n### global constraints\n- No behavior or functionality loss.\n- Preserve public API contracts unless explicitly approved.\n- Keep provider request/response shapes and stream-part ordering stable.\n- Do not accept any coupling regression at the workspace gate.\n- Each slice must be validated with tests and quant_metrics snapshot/compare.\n\n### definition of done\n- OpenAI failure semantics match expected non-stream and stream parity behavior.\n- Transport initialization no longer panics on client build failure paths.\n- Google and Google Vertex duplicated stacks are consolidated behind shared modules with parity maintained.\n- Provider bootstrap and error-mapping duplication are reduced with equivalent behavior.\n- Path-based module boundary debt is reduced enough for coupling metrics to remain trustworthy for future slices.\n- quant_metrics compare verdict remains improves or no-regression for accepted slices.\n\n### plan assumptions\n- Current parity gap matrix for OpenAI responses reflects active known gaps.\n- inventory-based provider registration remains required.\n- Refactors can be staged incrementally without full rewrite.\n\n### plan risks\n- Provider parity regressions caused by shared-module extraction.\n- Hidden differences between Google GenAI and Google Vertex behavior during consolidation.\n- Coupling metric fidelity is partially limited by current path-based module structure.\n\n## bead-local scope\nbead key: google-vertex-language-model-core\ntask: Refactor duplicated language_model streaming and mapping logic into shared core with provider-specific strategy hooks.\nintegration points touched: ip-google-stack-consolidation\nwhy: Major source of maintenance drift and repeated bug fixes.\n\n### deliverables\n- Shared language-model core path\n- Provider-specific strategy implementations for auth/scope/url differences\n\n### acceptance\n- Google and Vertex tests pass with shared core\n- Stream part ordering and metadata semantics remain stable\n\n### dependency intent\n- google-vertex-shared-primitives\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:2510105 picked-at=2026-02-14T02:55:39Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-r9t.lock","status":"closed","priority":1,"issue_type":"feature","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T18:24:56.393560524-08:00","created_by":"pomterre","updated_at":"2026-02-13T19:01:23.221370991-08:00","closed_at":"2026-02-13T19:01:23.221370991-08:00","close_reason":"Consolidated Google and Vertex streaming language-model core into shared stream_core with provider-scope strategy hook for metadata namespace; both providers now call the shared stream mapper and tests pass.","labels":["architecture","loc-reduction","streaming"],"dependencies":[{"issue_id":"ai-sdk-rs-r9t","depends_on_id":"ai-sdk-rs-bfg","type":"blocks","created_at":"2026-02-13T18:25:44.68369265-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-v4c","title":"Implement appendix sdk normalization in modrepo appendix export","description":"# atomic bead\n\n## full plan snapshot\nplan title: Provider Expansion Plan: Catalog-Normalized + ai-sdk-rs Registry Coverage\nplan summary: Normalize provider sdk labels at the Nodecode gateway appendix source and expand ai-sdk-rs provider routing so catalog providers resolve through existing implementations, starting with explicit Groq coverage and common OpenAI-compatible provider IDs.\n\n### integration points (complete map)\n- [ip-upstream-catalog-normalization] Gateway appendix sdk normalization\n  purpose: Guarantee emitted provider sdk values are canonical for ai-sdk-rs routing.\n  integration contract: appendix providers[].sdk values must be from the ai-sdk-rs-supported sdk set and preserve groq as groq.\n- [ip-openai-compatible-provider-id-routing] OpenAI-compatible provider-id routing in ai-sdk-rs\n  purpose: Resolve common provider IDs directly to OpenAI-compatible builder behavior.\n  integration contract: sdk_type_from_id and registry coverage must include target provider IDs with deterministic routing.\n- [ip-groq-routing] Explicit Groq routing support\n  purpose: Route sdk_type=groq catalog providers through a supported builder path.\n  integration contract: Groq provider definition must build a language model without fallback ambiguity.\n- [ip-nodecode-catalog-path] Nodecode catalog path precedence\n  purpose: Align runtime catalog reads to ~/.cache/nodecode/providers/catalog.json with safe fallback.\n  integration contract: path/env precedence is deterministic and test-covered for both nodecode and legacy paths.\n- [ip-fixture-parity-coverage] Fixture-based parity coverage for expanded provider IDs\n  purpose: Prevent request/stream/error mapping regressions while widening provider ID coverage.\n  integration contract: offline fixtures assert request serialization, stream normalization, and error mapping invariants.\n\n### global constraints\n- Preserve existing provider behavior for openai/anthropic/google/google-vertex/amazon-bedrock.\n- Use fixture-based parity tests only; no live provider API calls.\n- Keep Rust changes limited to currently representable interfaces: language, embeddings, images.\n- Any provider sdk label emitted by appendix must be parseable and routable by ai-sdk-rs without ad-hoc fallbacks.\n\n### definition of done\n- Gateway appendix export emits normalized sdk labels compatible with ai-sdk-rs.\n- ai-sdk-rs resolves and builds models for Groq and selected OpenAI-compatible provider IDs.\n- Capabilities loader prefers ~/.cache/nodecode/providers/catalog.json with env override precedence.\n- New regression tests cover sdk normalization, registry resolution, path precedence, and provider parity fixtures.\n- Dependency graph has no cycles and all created beads are ready/traceable.\n\n### plan assumptions\n- Catalog source of truth is generated in ~/nodecode-gateway-1 via modrepo appendix tooling.\n- Provider-specific TS surfaces without Rust interfaces (for example transcription/reranking) are explicitly deferred.\n- openai-compatible Rust implementation remains the shared execution path for aliased provider IDs.\n\n### plan risks\n- Unnormalized sdk labels can silently break provider dispatch at runtime.\n- Alias expansion may regress provider option scoping if provider_scope_name handling drifts.\n- Cache path changes can cause environment-specific behavior if precedence is not tested.\n\n## bead-local scope\nbead key: gateway-appendix-sdk-normalization\ntask: Update ~/nodecode-gateway-1/modrepo/cli/src/commands/appendix.rs to normalize outgoing provider sdk labels to ai-sdk-rs-compatible values, preserving groq.\nintegration points touched: ip-upstream-catalog-normalization\nwhy: Provider routing reliability depends on source normalization rather than downstream ad-hoc mapping.\n\n### deliverables\n- normalization mapping implementation in appendix export path\n- explicit handling for bedrock/google-generative-ai/mistral/xai/openai-compatible families\n\n### acceptance\n- appendix export never emits unsupported sdk labels for routing\n- groq remains emitted as groq\n\n### dependency intent\n- provider-expansion-baseline-matrix\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:3049663 picked-at=2026-02-14T04:43:05Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-v4c.lock","status":"in_progress","priority":0,"issue_type":"feature","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T20:33:08.826453444-08:00","created_by":"pomterre","updated_at":"2026-02-13T20:43:05.914525373-08:00","labels":["appendix","gateway"],"dependencies":[{"issue_id":"ai-sdk-rs-v4c","depends_on_id":"ai-sdk-rs-am3","type":"blocks","created_at":"2026-02-13T20:33:09.314987496-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-vvn","title":"Add request serialization tests for function tool strict passthrough","description":"# atomic bead\n\n## full plan snapshot\nplan title: OpenAI Responses parity remediation for ai-sdk-rs\nplan summary: Close behavior drift between ai-sdk-rs OpenAI Responses provider and /home/mike/ai by fixing non-stream error propagation, response.failed streaming finish semantics, usage detail mapping, and function tool strict passthrough, with fixture-based regression coverage.\n\n### integration points (complete map)\n- [ip-nonstream-response-error] Non-stream response.error propagation\n  purpose: Ensure do_generate fails when OpenAI response payload contains top-level response.error, matching TS provider behavior.\n  integration contract: Given a response with response.error, ai-sdk-rs must surface an error instead of returning normal content.\n- [ip-stream-failed-finish] Streaming response.failed finish semantics\n  purpose: Align response.failed/error stream terminal behavior with TS finish reason semantics.\n  integration contract: For error/failed stream trajectories, finish reason and terminal parts match TS parity expectations.\n- [ip-usage-details] Usage details normalization\n  purpose: Map nested usage detail fields (cached and reasoning token details) into normalized usage outputs.\n  integration contract: input_tokens_details.cached_tokens and output_tokens_details.reasoning_tokens are preserved in normalized usage fields.\n- [ip-function-tool-strict] Function tool strict passthrough\n  purpose: Preserve function tool strict mode when building Responses API request bodies.\n  integration contract: Function tools include strict=true/false when specified and omit strict when unspecified.\n\n### global constraints\n- Scope is existing provider only: openai responses path in ai-sdk-rs.\n- Use fixture-based tests; no live provider API calls.\n- Preserve normalized stream contract in crates/sdk-types/src/v2.rs and avoid introducing incompatible stream part shapes.\n- Keep fixes localized to OpenAI provider/core mapping paths unless type support requires shared sdk-types changes.\n\n### definition of done\n- All four parity gaps are covered by deterministic tests and passing implementations.\n- OpenAI provider targeted tests pass for both request serialization and stream normalization paths.\n- No dependency cycles exist in bead graph and execution order reflects blocker intent.\n- Final parity note lists resolved gaps and any residual risk with file-level references.\n\n### plan assumptions\n- Current TS behavior in /home/mike/ai/packages/openai/src/responses is the parity baseline.\n- OpenAI fixture corpus in crates/providers/openai/tests/fixtures is authoritative for offline validation.\n- Rust stream finish behavior should align with TS snapshots for response.failed/error trajectories.\n\n### plan risks\n- Adjusting finish semantics may affect downstream consumers that implicitly depended on previous finish reasons.\n- Adding strict support may require shared type updates that can affect other providers if not isolated.\n- Usage mapping changes could unintentionally alter reported token accounting in unrelated tests.\n\n## bead-local scope\nbead key: test-function-tool-strict-passthrough\ntask: Add failing request-body serialization tests covering function tool strict=true, strict=false, and strict omitted behavior.\nintegration points touched: ip-function-tool-strict\nwhy: n/a\n\n### deliverables\n- New test cases in crates/providers/openai/tests/responses_language_model_tests.rs\n- Assertions on serialized tools[].strict field behavior\n\n### acceptance\n- Tests fail on current serialization behavior\n- Assertions cover true/false/unspecified strict scenarios\n\n### dependency intent\n- openai-parity-gap-matrix\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:2367372 picked-at=2026-02-14T02:19:04Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-vvn.lock","status":"closed","priority":1,"issue_type":"task","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T18:02:05.342158217-08:00","created_by":"pomterre","updated_at":"2026-02-13T18:22:29.601791065-08:00","closed_at":"2026-02-13T18:22:29.601791065-08:00","close_reason":"Added strict passthrough regression tests for function tools: strict=true/false must serialize and strict omitted must remain absent.","labels":["openai","parity","serialization","tests"],"dependencies":[{"issue_id":"ai-sdk-rs-vvn","depends_on_id":"ai-sdk-rs-8uf","type":"blocks","created_at":"2026-02-13T18:02:44.013149285-08:00","created_by":"pomterre"}]}
{"id":"ai-sdk-rs-zl5","title":"Run OpenAI parity regression gate and publish closure note","description":"# atomic bead\n\n## full plan snapshot\nplan title: OpenAI Responses parity remediation for ai-sdk-rs\nplan summary: Close behavior drift between ai-sdk-rs OpenAI Responses provider and /home/mike/ai by fixing non-stream error propagation, response.failed streaming finish semantics, usage detail mapping, and function tool strict passthrough, with fixture-based regression coverage.\n\n### integration points (complete map)\n- [ip-nonstream-response-error] Non-stream response.error propagation\n  purpose: Ensure do_generate fails when OpenAI response payload contains top-level response.error, matching TS provider behavior.\n  integration contract: Given a response with response.error, ai-sdk-rs must surface an error instead of returning normal content.\n- [ip-stream-failed-finish] Streaming response.failed finish semantics\n  purpose: Align response.failed/error stream terminal behavior with TS finish reason semantics.\n  integration contract: For error/failed stream trajectories, finish reason and terminal parts match TS parity expectations.\n- [ip-usage-details] Usage details normalization\n  purpose: Map nested usage detail fields (cached and reasoning token details) into normalized usage outputs.\n  integration contract: input_tokens_details.cached_tokens and output_tokens_details.reasoning_tokens are preserved in normalized usage fields.\n- [ip-function-tool-strict] Function tool strict passthrough\n  purpose: Preserve function tool strict mode when building Responses API request bodies.\n  integration contract: Function tools include strict=true/false when specified and omit strict when unspecified.\n\n### global constraints\n- Scope is existing provider only: openai responses path in ai-sdk-rs.\n- Use fixture-based tests; no live provider API calls.\n- Preserve normalized stream contract in crates/sdk-types/src/v2.rs and avoid introducing incompatible stream part shapes.\n- Keep fixes localized to OpenAI provider/core mapping paths unless type support requires shared sdk-types changes.\n\n### definition of done\n- All four parity gaps are covered by deterministic tests and passing implementations.\n- OpenAI provider targeted tests pass for both request serialization and stream normalization paths.\n- No dependency cycles exist in bead graph and execution order reflects blocker intent.\n- Final parity note lists resolved gaps and any residual risk with file-level references.\n\n### plan assumptions\n- Current TS behavior in /home/mike/ai/packages/openai/src/responses is the parity baseline.\n- OpenAI fixture corpus in crates/providers/openai/tests/fixtures is authoritative for offline validation.\n- Rust stream finish behavior should align with TS snapshots for response.failed/error trajectories.\n\n### plan risks\n- Adjusting finish semantics may affect downstream consumers that implicitly depended on previous finish reasons.\n- Adding strict support may require shared type updates that can affect other providers if not isolated.\n- Usage mapping changes could unintentionally alter reported token accounting in unrelated tests.\n\n## bead-local scope\nbead key: openai-parity-regression-gate\ntask: Run the targeted OpenAI test suite after fixes, verify all parity expectations are green, and publish a concise closure note with any residual risk or follow-up beads.\nintegration points touched: ip-nonstream-response-error, ip-stream-failed-finish, ip-usage-details, ip-function-tool-strict\nwhy: A final gate bead prevents partial completion and ensures parity outcomes are explicitly verified.\n\n### deliverables\n- Targeted test run output summary\n- Closure note with resolved gaps and any follow-up issue IDs\n\n### acceptance\n- All newly added parity regressions pass\n- Any remaining drift is tracked as explicit follow-up bead(s)\n\n### dependency intent\n- fix-do-generate-response-error\n- fix-stream-response-failed-finish\n- fix-usage-nested-details-mapping\n- fix-function-tool-strict-passthrough\n\n## execution guardrail\npreserve plan compatibility invariants while delivering only this bead's scope.\n","notes":"[beads-claim] picked-by=mike@archlinux:2527972 picked-at=2026-02-14T03:00:40Z selector=next_unblocking.sh attempt=1 lock=/home/mike/nodecode/ai-sdk-rs/.beads/agent-picks/ai-sdk-rs-zl5.lock\nclosure-note (2026-02-14): OpenAI parity regression gate passed.\n- Verified ip-nonstream-response-error + ip-function-tool-strict + non-stream usage mapping via `cargo test responses_language_model_tests -- --nocapture` (15 passed), covering `crates/providers/openai/tests/responses_language_model_tests.rs`.\n- Verified ip-stream-failed-finish + stream usage mapping via `cargo test stream_fixture_tests -- --nocapture` (17 passed), covering `crates/providers/openai/tests/stream_fixture_tests.rs`.\n- Residual risk: unrelated compile warnings remain in `crates/providers/google-vertex/src/language_model.rs` and `crates/providers/google-vertex/src/options.rs` (dead_code), but no OpenAI parity failures observed.\n- Follow-up beads: none required for OpenAI parity at this gate.","status":"closed","priority":0,"issue_type":"task","assignee":"pomterre","owner":"39233499+pomterre@users.noreply.github.com","created_at":"2026-02-13T18:02:33.914578082-08:00","created_by":"pomterre","updated_at":"2026-02-13T19:02:13.262524508-08:00","closed_at":"2026-02-13T19:02:13.262524508-08:00","close_reason":"OpenAI parity regression gate passed: responses_language_model_tests (15/15) and stream_fixture_tests (17/17); closure note recorded","labels":["gate","openai","parity"],"dependencies":[{"issue_id":"ai-sdk-rs-zl5","depends_on_id":"ai-sdk-rs-pfo","type":"blocks","created_at":"2026-02-13T18:02:44.098241908-08:00","created_by":"pomterre"},{"issue_id":"ai-sdk-rs-zl5","depends_on_id":"ai-sdk-rs-7jd","type":"blocks","created_at":"2026-02-13T18:02:44.142478335-08:00","created_by":"pomterre"},{"issue_id":"ai-sdk-rs-zl5","depends_on_id":"ai-sdk-rs-7td","type":"blocks","created_at":"2026-02-13T18:02:44.184485227-08:00","created_by":"pomterre"},{"issue_id":"ai-sdk-rs-zl5","depends_on_id":"ai-sdk-rs-jmd","type":"blocks","created_at":"2026-02-13T18:02:44.228136297-08:00","created_by":"pomterre"}]}
