# OpenAI Responses Parity Gap Matrix (bead `ai-sdk-rs-8uf`)

Scope baseline: `/home/mike/ai/packages/openai/src/responses`

| Integration point | Rust target files (current state) | TS baseline lines | Fixture + test inventory | Expected parity behavior |
| --- | --- | --- | --- | --- |
| `ip-nonstream-response-error` | `crates/providers/openai/src/responses/language_model.rs:214` (`do_generate`), `crates/providers/openai/src/responses/language_model.rs:237` (non-stream JSON response path currently returns success payload without checking `response.error`) | `/home/mike/ai/packages/openai/src/responses/openai-responses-language-model.ts:449` (throws on `response.error`), `/home/mike/ai/packages/openai/src/responses/openai-responses-language-model.test.ts:1440` (error regression) | Existing fixture: `crates/providers/openai/tests/fixtures/openai-error.1.json`. Rust test target: `crates/providers/openai/tests/responses_language_model_tests.rs` (add `do_generate` error regression). | If the response body contains top-level `error`, `do_generate` must return an error, not `GenerateResponse`. |
| `ip-stream-failed-finish` | `crates/providers/openai/src/responses/language_model.rs:2101` (currently treats `response.failed` like completed/incomplete), `crates/providers/openai/src/responses/language_model.rs:3052` (`map_finish_reason`), `crates/providers/openai/src/responses/language_model.rs:3040` (fallback `Stop`) | `/home/mike/ai/packages/openai/src/responses/openai-responses-language-model.ts:923` (default stream finish is `other`), `/home/mike/ai/packages/openai/src/responses/openai-responses-language-model.ts:1760` (emits error chunk), `/home/mike/ai/packages/openai/src/responses/openai-responses-language-model.ts:1807` (`response.failed` is not a finished chunk), `/home/mike/ai/packages/openai/src/responses/openai-responses-language-model.test.ts:4713` (error stream snapshot) | Existing fixture: `crates/providers/openai/tests/fixtures/openai-error.1.chunks.txt`. Rust test target: `crates/providers/openai/tests/stream_fixture_tests.rs:478` (extend from presence checks to finish-reason assertions). Type reference: `crates/sdk-types/src/v2.rs:420` (`FinishReason`). | For failed/error stream trajectories, emitted finish semantics must match TS parity expectations (including finish reason for `openai-error.1`). |
| `ip-usage-details` | `crates/providers/openai/src/responses/language_model.rs:356` (`parse_openai_usage`), `crates/providers/openai/src/responses/language_model.rs:251` (`do_generate` usage mapping currently sets `cached_input_tokens` only), type target `crates/sdk-types/src/v2.rs:437` (`Usage.reasoning_tokens`, `Usage.cached_input_tokens`) | `/home/mike/ai/packages/openai/src/responses/convert-openai-responses-usage.ts:34` (maps `input_tokens_details.cached_tokens`), `/home/mike/ai/packages/openai/src/responses/convert-openai-responses-usage.ts:37` (maps `output_tokens_details.reasoning_tokens`), `/home/mike/ai/packages/openai/src/responses/openai-responses-language-model.test.ts:184` (usage snapshot) | Existing fixtures containing nested usage details: `crates/providers/openai/tests/fixtures/openai-local-shell-tool.1.json`, `crates/providers/openai/tests/fixtures/openai-local-shell-tool.1.chunks.txt`, `crates/providers/openai/tests/fixtures/openai-mcp-tool-approval.1.json`. Rust test targets: `crates/providers/openai/tests/responses_language_model_tests.rs`, `crates/providers/openai/tests/stream_fixture_tests.rs`. | Preserve nested usage details by mapping `input_tokens_details.cached_tokens -> Usage.cached_input_tokens` and `output_tokens_details.reasoning_tokens -> Usage.reasoning_tokens`. |
| `ip-function-tool-strict` | `crates/providers/openai/src/responses/language_model.rs:4124` (function tool request JSON creation currently omits `strict`), `crates/sdk-types/src/v2.rs:334` (`FunctionTool` shape currently has no `strict` field) | `/home/mike/ai/packages/openai/src/responses/openai-responses-prepare-tools.ts:50` (function tool mapping), `/home/mike/ai/packages/openai/src/responses/openai-responses-prepare-tools.ts:56` (conditional strict passthrough), `/home/mike/ai/packages/openai/src/responses/openai-responses-prepare-tools.test.ts:6` (strict true/false/undefined regressions), `/home/mike/ai/packages/openai/src/responses/openai-responses-language-model.test.ts:2168` (serialized strict in response fixtures) | Rust request serialization test target: `crates/providers/openai/tests/responses_language_model_tests.rs:184` (`request_body_includes_provider_tools_and_tool_choice`) plus dedicated strict true/false/omitted assertions (fixture-backed or inline). | Function tools must pass through strict mode exactly: include `strict: true/false` when set; omit `strict` when unset. |

## Downstream bead mapping

- `ai-sdk-rs-h3z` -> `ip-nonstream-response-error`
- `ai-sdk-rs-9jf` -> `ip-stream-failed-finish`
- `ai-sdk-rs-62d` -> `ip-usage-details`
- `ai-sdk-rs-vvn` -> `ip-function-tool-strict`

## Notes for execution order

- Apply non-stream/stream error semantics before tightening snapshots, because stream finish assertions depend on final mapping policy.
- Add `FunctionTool.strict` in `sdk-types` before request-body assertions for strict passthrough.
